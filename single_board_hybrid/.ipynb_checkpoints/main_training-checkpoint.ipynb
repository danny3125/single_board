{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU name: GeForce RTX 2080 Ti, gpu_id: 0\n",
      "cuda\n",
      "pytorch version =  1.10.0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import time\n",
    "import argparse\n",
    "\n",
    "import os\n",
    "import datetime\n",
    "\n",
    "from torch.distributions.categorical import Categorical\n",
    "\n",
    "# visualization \n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "device = torch.device(\"cpu\"); gpu_id = -1 # select CPU\n",
    "\n",
    "gpu_id = '0' # select a single GPU  \n",
    "#gpu_id = '2,3' # select multiple GPUs  \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(gpu_id)  \n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print('GPU name: {:s}, gpu_id: {:s}'.format(torch.cuda.get_device_name(0),gpu_id))   \n",
    "    \n",
    "print(device)\n",
    "print('pytorch version = ',torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from torch.optim import lr_scheduler\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm_notebook\n",
    "class TransEncoderNet(nn.Module):\n",
    "    \"\"\"\n",
    "    Encoder network based on self-attention transformer\n",
    "    Inputs :  \n",
    "      h of size      (bsz, nb_nodes, dim_emb)    batch of input cities\n",
    "    Outputs :  \n",
    "      h of size      (bsz, nb_nodes, dim_emb)    batch of encoded cities\n",
    "      score of size  (bsz, nb_nodes, nb_nodes+1) batch of attention scores\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, nb_layers, dim_emb, nb_heads, dim_ff, batchnorm):\n",
    "        super(TransEncoderNet, self).__init__()\n",
    "        assert dim_emb == nb_heads* (dim_emb//nb_heads) # check if dim_emb is divisible by nb_heads\n",
    "        self.MHA_layers = nn.ModuleList( [nn.MultiheadAttention(dim_emb, nb_heads) for _ in range(nb_layers)] )\n",
    "        self.linear1_layers = nn.ModuleList( [nn.Linear(dim_emb, dim_ff) for _ in range(nb_layers)] )\n",
    "        self.linear2_layers = nn.ModuleList( [nn.Linear(dim_ff, dim_emb) for _ in range(nb_layers)] )   \n",
    "        if batchnorm:\n",
    "            self.norm1_layers = nn.ModuleList( [nn.BatchNorm1d(dim_emb) for _ in range(nb_layers)] )\n",
    "            self.norm2_layers = nn.ModuleList( [nn.BatchNorm1d(dim_emb) for _ in range(nb_layers)] )\n",
    "        else:\n",
    "            self.norm1_layers = nn.ModuleList( [nn.LayerNorm(dim_emb) for _ in range(nb_layers)] )\n",
    "            self.norm2_layers = nn.ModuleList( [nn.LayerNorm(dim_emb) for _ in range(nb_layers)] )\n",
    "        self.nb_layers = nb_layers\n",
    "        self.nb_heads = nb_heads\n",
    "        self.batchnorm = batchnorm\n",
    "        \n",
    "    def forward(self, h):      \n",
    "        # PyTorch nn.MultiheadAttention requires input size (seq_len, bsz, dim_emb) \n",
    "        h = h.transpose(0,1) # size(h)=(nb_nodes, bsz, dim_emb)  \n",
    "        # L layers\n",
    "        for i in range(self.nb_layers):\n",
    "            h_rc = h # residual connection, size(h_rc)=(nb_nodes, bsz, dim_emb)\n",
    "            h, score = self.MHA_layers[i](h, h, h) # size(h)=(nb_nodes, bsz, dim_emb), size(score)=(bsz, nb_nodes, nb_nodes)\n",
    "            # add residual connection\n",
    "            \n",
    "            h = h_rc + h # size(h)=(nb_nodes, bsz, dim_emb)\n",
    "            if self.batchnorm:\n",
    "                # Pytorch nn.BatchNorm1d requires input size (bsz, dim, seq_len)\n",
    "                h = h.permute(1,2,0).contiguous() # size(h)=(bsz, dim_emb, nb_nodes)\n",
    "                h = self.norm1_layers[i](h)       # size(h)=(bsz, dim_emb, nb_nodes)\n",
    "                h = h.permute(2,0,1).contiguous() # size(h)=(nb_nodes, bsz, dim_emb)\n",
    "            else:\n",
    "                h = self.norm1_layers[i](h)       # size(h)=(nb_nodes, bsz, dim_emb) \n",
    "            # feedforward\n",
    "            h_rc = h # residual connection\n",
    "            h = self.linear2_layers[i](torch.relu(self.linear1_layers[i](h)))\n",
    "            h = h_rc + h # size(h)=(nb_nodes, bsz, dim_emb)\n",
    "            if self.batchnorm:\n",
    "                h = h.permute(1,2,0).contiguous() # size(h)=(bsz, dim_emb, nb_nodes)\n",
    "                h = self.norm2_layers[i](h)       # size(h)=(bsz, dim_emb, nb_nodes)\n",
    "                h = h.permute(2,0,1).contiguous() # size(h)=(nb_nodes, bsz, dim_emb)\n",
    "            else:\n",
    "                h = self.norm2_layers[i](h) # size(h)=(nb_nodes, bsz, dim_emb)\n",
    "        # Transpose h\n",
    "        h = h.transpose(0,1) # size(h)=(bsz, nb_nodes, dim_emb)\n",
    "        return h, score\n",
    "    \n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, n_hidden):\n",
    "        super(Attention, self).__init__()\n",
    "        self.size = 0\n",
    "        self.batch_size = 0\n",
    "        self.dim = n_hidden\n",
    "        \n",
    "        v  = torch.FloatTensor(n_hidden)\n",
    "        self.v  = nn.Parameter(v)\n",
    "        self.v.data.uniform_(-1/math.sqrt(n_hidden), 1/math.sqrt(n_hidden))\n",
    "        \n",
    "        # parameters for pointer attention\n",
    "        self.Wref = nn.Linear(n_hidden, n_hidden)\n",
    "        self.Wq = nn.Linear(n_hidden, n_hidden)\n",
    "    \n",
    "    \n",
    "    def forward(self, q, ref):       # query and reference\n",
    "        self.batch_size = q.size(0)\n",
    "        self.size = int(ref.size(0) / self.batch_size)\n",
    "        q = self.Wq(q)     # (B, dim)\n",
    "        ref = self.Wref(ref)\n",
    "        ref = ref.view(self.batch_size, self.size, self.dim)  # (B, size, dim)\n",
    "        \n",
    "        q_ex = q.unsqueeze(1).repeat(1, self.size, 1) # (B, size, dim)\n",
    "        # v_view: (B, dim, 1)\n",
    "        v_view = self.v.unsqueeze(0).expand(self.batch_size, self.dim).unsqueeze(2)\n",
    "        \n",
    "        # (B, size, dim) * (B, dim, 1)\n",
    "        u = torch.bmm(torch.tanh(q_ex + ref), v_view).squeeze(2)\n",
    "        \n",
    "        return u, ref\n",
    "    \n",
    "class LSTM(nn.Module):\n",
    "    def __init__(self, n_hidden):\n",
    "        super(LSTM, self).__init__()\n",
    "        \n",
    "        # parameters for input gate\n",
    "        self.Wxi = nn.Linear(n_hidden, n_hidden)    # W(xt)\n",
    "        self.Whi = nn.Linear(n_hidden, n_hidden)    # W(ht)\n",
    "        self.wci = nn.Linear(n_hidden, n_hidden)    # w(ct)\n",
    "        \n",
    "        # parameters for forget gate\n",
    "        self.Wxf = nn.Linear(n_hidden, n_hidden)    # W(xt)\n",
    "        self.Whf = nn.Linear(n_hidden, n_hidden)    # W(ht)\n",
    "        self.wcf = nn.Linear(n_hidden, n_hidden)    # w(ct)\n",
    "        \n",
    "        # parameters for cell gate\n",
    "        self.Wxc = nn.Linear(n_hidden, n_hidden)    # W(xt)\n",
    "        self.Whc = nn.Linear(n_hidden, n_hidden)    # W(ht)\n",
    "        \n",
    "        # parameters for forget gate\n",
    "        self.Wxo = nn.Linear(n_hidden, n_hidden)    # W(xt)\n",
    "        self.Who = nn.Linear(n_hidden, n_hidden)    # W(ht)\n",
    "        self.wco = nn.Linear(n_hidden, n_hidden)    # w(ct)\n",
    "    \n",
    "    \n",
    "    def forward(self, x, h, c):       # query and reference\n",
    "        \n",
    "        # input gate\n",
    "        i = torch.sigmoid(self.Wxi(x) + self.Whi(h) + self.wci(c))\n",
    "        # forget gate\n",
    "        f = torch.sigmoid(self.Wxf(x) + self.Whf(h) + self.wcf(c))\n",
    "        # cell gate\n",
    "        c = f * c + i * torch.tanh(self.Wxc(x) + self.Whc(h))\n",
    "        # output gate\n",
    "        o = torch.sigmoid(self.Wxo(x) + self.Who(h) + self.wco(c))\n",
    "        \n",
    "        h = o * torch.tanh(c)\n",
    "        \n",
    "        return h, c\n",
    "\n",
    "class HPN(nn.Module):\n",
    "    def __init__(self, n_feature, n_hidden):\n",
    "\n",
    "        super(HPN, self).__init__()\n",
    "        self.city_size = 0\n",
    "        self.batch_size = 0\n",
    "        self.dim = n_hidden\n",
    "        \n",
    "        # lstm for first turn\n",
    "        #self.lstm0 = nn.LSTM(n_hidden, n_hidden)\n",
    "        \n",
    "        # pointer layer\n",
    "        self.pointer = Attention(n_hidden)\n",
    "        self.TransPointer = Attention(n_hidden)\n",
    "        \n",
    "        # lstm encoder\n",
    "        self.encoder = LSTM(n_hidden)\n",
    "        \n",
    "        # trainable first hidden input\n",
    "        h0 = torch.FloatTensor(n_hidden)\n",
    "        c0 = torch.FloatTensor(n_hidden)\n",
    "        # trainable latent variable coefficient\n",
    "        print('here') \n",
    "        alpha = torch.ones(1).cuda()       \n",
    "        self.h0 = nn.Parameter(h0)\n",
    "        self.c0 = nn.Parameter(c0)\n",
    "        \n",
    "        self.alpha = nn.Parameter(alpha)\n",
    "        self.h0.data.uniform_(-1/math.sqrt(n_hidden), 1/math.sqrt(n_hidden))\n",
    "        self.c0.data.uniform_(-1/math.sqrt(n_hidden), 1/math.sqrt(n_hidden))\n",
    "        \n",
    "        r1 = torch.ones(1)\n",
    "        r2 = torch.ones(1)\n",
    "        r3 = torch.ones(1)\n",
    "        self.r1 = nn.Parameter(r1)\n",
    "        self.r2 = nn.Parameter(r2)\n",
    "        self.r3 = nn.Parameter(r3)\n",
    "        \n",
    "        # embedding\n",
    "        self.embedding_x = nn.Linear(n_feature, n_hidden)\n",
    "        self.embedding_all = nn.Linear(n_feature, n_hidden)\n",
    "        self.Transembedding_all = TransEncoderNet(6, 128, 8, 512, batchnorm=True)#6,128,8,512\n",
    "        \n",
    "        # vector to start decoding \n",
    "        self.start_placeholder = nn.Parameter(torch.randn(n_hidden))\n",
    "        \n",
    "        # weights for GNN\n",
    "        self.W1 = nn.Linear(n_hidden, n_hidden)\n",
    "        self.W2 = nn.Linear(n_hidden, n_hidden)\n",
    "        self.W3 = nn.Linear(n_hidden, n_hidden)\n",
    "        \n",
    "        # aggregation function for GNN\n",
    "        self.agg_1 = nn.Linear(n_hidden, n_hidden)\n",
    "        self.agg_2 = nn.Linear(n_hidden, n_hidden)\n",
    "        self.agg_3 = nn.Linear(n_hidden, n_hidden)\n",
    "    \n",
    "    \n",
    "    def forward(self,context,Transcontext, x, X_all, mask, h=None, c=None, latent=None):\n",
    "        '''\n",
    "        Inputs (B: batch size, size: city size, dim: hidden dimension)\n",
    "        \n",
    "        x: current city coordinate (B, 2)\n",
    "        X_all: all cities' cooridnates (B, size, 2)\n",
    "        mask: mask visited cities\n",
    "        h: hidden variable (B, dim)\n",
    "        c: cell gate (B, dim)\n",
    "        latent: latent pointer vector from previous layer (B, size, dim)\n",
    "        \n",
    "        Outputs\n",
    "        \n",
    "        softmax: probability distribution of next city (B, size)\n",
    "        h: hidden variable (B, dim)\n",
    "        c: cell gate (B, dim)\n",
    "        latent_u: latent pointer vector for next layer\n",
    "        '''\n",
    "        \n",
    "        self.batch_size = X_all.size(0)\n",
    "        self.city_size = X_all.size(1)\n",
    "        \n",
    "        # Check if this the first iteration loop\n",
    "        if h is None or c is None:\n",
    "            x          = self.start_placeholder    \n",
    "            context = self.embedding_all(X_all)\n",
    "            Transcontext,_ = self.Transembedding_all(context)\n",
    "            \n",
    "            # =============================\n",
    "            # graph neural network encoder\n",
    "            # =============================\n",
    "\n",
    "            # (B, size, dim)\n",
    "            context = context.reshape(-1, self.dim)\n",
    "            Transcontext = Transcontext.reshape(-1, self.dim)\n",
    "\n",
    "            context = self.r1 * self.W1(context)\\\n",
    "                + (1-self.r1) * F.relu(self.agg_1(context/(self.city_size-1)))\n",
    "\n",
    "            context = self.r2 * self.W2(context)\\\n",
    "                + (1-self.r2) * F.relu(self.agg_2(context/(self.city_size-1)))\n",
    "\n",
    "            context = self.r3 * self.W3(context)\\\n",
    "                + (1-self.r3) * F.relu(self.agg_3(context/(self.city_size-1)))\n",
    "            h0 = self.h0.unsqueeze(0).expand(self.batch_size, self.dim)\n",
    "            c0 = self.c0.unsqueeze(0).expand(self.batch_size, self.dim)\n",
    "\n",
    "            h0 = h0.unsqueeze(0).contiguous()\n",
    "            c0 = c0.unsqueeze(0).contiguous()\n",
    "            \n",
    "            # let h0, c0 be the hidden variable of first turn\n",
    "            h = h0.squeeze(0)\n",
    "            c = c0.squeeze(0)\n",
    "        else:\n",
    "            x          = self.embedding_x(x)\n",
    "        # LSTM encoder\n",
    "        h, c = self.encoder(x, h, c)\n",
    "        # query vector\n",
    "        q = h\n",
    "        # pointer\n",
    "        u1, _ = self.pointer(q, context)\n",
    "        u2 ,_ = self.TransPointer(q,Transcontext)\n",
    "        # Avg Agg between the two attention vectors\n",
    "        u = torch.maximum(u1,u2)\n",
    "        latent_u = u.clone()\n",
    "        u = 10 * torch.tanh(u) + mask\n",
    "        return context,Transcontext,F.softmax(u, dim=1), h, c, latent_u"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# daniel rectangle feature handle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "'''\n",
    "This part I designed the rectangle-characterized TSP, that means for every step the agent walk through a corner,\n",
    "then he travel through the whole rectangle using zig-zag, finally he ends up at one of the rest corners of \n",
    "the rextangle, so, it equals the agent walk through three points at one step, in practice, I add three points into \n",
    "mask to make them unselectable.\n",
    "'''\n",
    "def rectangle_process(f_temp,idx,Y,Y0,mask,k,B,i,path_gazebo):\n",
    "    Y1 = Y[zero_to_bsz, idx.data].clone()\n",
    "    rectangle_inf = idx/4\n",
    "    feature_table = f_temp.outcorner_getout(rectangle_inf,B)\n",
    "    feature_table = torch.Tensor(feature_table).type(torch.long)\n",
    "    Y_corner = Y[zero_to_bsz, feature_table[:,0].data].clone()\n",
    "    if i % 200 == 0:\n",
    "        path_gazebo.append([idx.data[0].tolist(),feature_table[:,0].data[0].tolist()])\n",
    "    if k ==0:\n",
    "        reward = 0\n",
    "    if k > 0:\n",
    "        reward = torch.sum((Y1 - Y0) ** 2, dim=1) ** 0.5\n",
    "        reward += torch.sum((Y_corner - Y1) ** 2, dim=1) ** 0.5   \n",
    "    mask[zero_to_bsz, idx.data] += -np.inf\n",
    "    mask[zero_to_bsz, feature_table[:,0].data] += -np.inf\n",
    "    mask[zero_to_bsz, feature_table[:,1].data ] += -np.inf    \n",
    "    mask[zero_to_bsz, feature_table[:,2].data ] += -np.inf \n",
    "    return reward, Y_corner, Y_corner, path_gazebo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================\n",
      "prepare to train\n",
      "======================\n",
      "Hyper parameters:\n",
      "learning rate 0.0001\n",
      "batch size 256\n",
      "steps 2500\n",
      "epoch 100\n",
      "======================\n",
      "here\n",
      "here\n",
      "epoch:0, batch:50/2500, reward:5174.3173828125\n",
      "epoch:0, batch:100/2500, reward:4770.83642578125\n",
      "epoch:0, batch:150/2500, reward:4211.4111328125\n",
      "epoch:0, batch:200/2500, reward:4441.8896484375\n",
      "epoch:0, batch:250/2500, reward:4644.1201171875\n",
      "epoch:0, batch:300/2500, reward:4218.0244140625\n",
      "epoch:0, batch:350/2500, reward:4057.454345703125\n",
      "epoch:0, batch:400/2500, reward:4061.0751953125\n",
      "epoch:0, batch:450/2500, reward:3892.18798828125\n",
      "epoch:0, batch:500/2500, reward:4148.2099609375\n",
      "epoch:0, batch:550/2500, reward:3945.40283203125\n",
      "epoch:0, batch:600/2500, reward:4044.068115234375\n",
      "epoch:0, batch:650/2500, reward:3948.2998046875\n",
      "epoch:0, batch:700/2500, reward:3822.203857421875\n",
      "epoch:0, batch:750/2500, reward:3919.52490234375\n",
      "epoch:0, batch:800/2500, reward:4121.73046875\n",
      "epoch:0, batch:850/2500, reward:3901.02783203125\n",
      "epoch:0, batch:900/2500, reward:3805.72509765625\n",
      "epoch:0, batch:950/2500, reward:3861.771240234375\n",
      "epoch:0, batch:1000/2500, reward:3808.3046875\n",
      "epoch:0, batch:1050/2500, reward:3902.13232421875\n",
      "epoch:0, batch:1100/2500, reward:3900.208984375\n",
      "epoch:0, batch:1150/2500, reward:3821.9365234375\n",
      "epoch:0, batch:1200/2500, reward:3787.70166015625\n",
      "epoch:0, batch:1250/2500, reward:3712.8818359375\n",
      "epoch:0, batch:1300/2500, reward:3749.152099609375\n",
      "epoch:0, batch:1350/2500, reward:3672.85498046875\n",
      "epoch:0, batch:1400/2500, reward:3606.01513671875\n",
      "epoch:0, batch:1450/2500, reward:3601.17919921875\n",
      "epoch:0, batch:1500/2500, reward:3686.6513671875\n",
      "epoch:0, batch:1550/2500, reward:3629.07373046875\n",
      "epoch:0, batch:1600/2500, reward:3747.1044921875\n",
      "epoch:0, batch:1650/2500, reward:3622.27734375\n",
      "epoch:0, batch:1700/2500, reward:3515.385986328125\n",
      "epoch:0, batch:1750/2500, reward:3750.343505859375\n",
      "epoch:0, batch:1800/2500, reward:3525.110595703125\n",
      "epoch:0, batch:1850/2500, reward:3856.321044921875\n",
      "epoch:0, batch:1900/2500, reward:3495.19482421875\n",
      "epoch:0, batch:1950/2500, reward:3924.439453125\n",
      "epoch:0, batch:2000/2500, reward:3684.381103515625\n",
      "epoch:0, batch:2050/2500, reward:3558.314453125\n",
      "epoch:0, batch:2100/2500, reward:3500.55078125\n",
      "epoch:0, batch:2150/2500, reward:3535.6513671875\n",
      "epoch:0, batch:2200/2500, reward:3471.8564453125\n",
      "epoch:0, batch:2250/2500, reward:3494.3955078125\n",
      "epoch:0, batch:2300/2500, reward:3664.39111328125\n",
      "epoch:0, batch:2350/2500, reward:3477.22607421875\n",
      "epoch:0, batch:2400/2500, reward:3568.070068359375\n",
      "epoch:0, batch:2450/2500, reward:3704.77392578125\n",
      "epoch:0, batch:2500/2500, reward:3709.50244140625\n",
      "Avg Actor 2764.1279296875 --- Avg Critic 10077.904296875\n",
      "My actor is going on the right road Hallelujah :) Updated\n",
      "Epoch: 0, epoch time: 74.957min, tot time: 0.052day, L_actor: 2764.128, L_critic: 10077.904, update: True\n",
      "Save Checkpoints\n",
      "epoch:1, batch:50/2500, reward:3384.722412109375\n",
      "epoch:1, batch:100/2500, reward:3329.80810546875\n",
      "epoch:1, batch:150/2500, reward:3246.630859375\n",
      "epoch:1, batch:200/2500, reward:3239.90673828125\n",
      "epoch:1, batch:250/2500, reward:3215.7783203125\n",
      "epoch:1, batch:300/2500, reward:3230.9365234375\n",
      "epoch:1, batch:350/2500, reward:3182.013671875\n",
      "epoch:1, batch:400/2500, reward:3131.95458984375\n",
      "epoch:1, batch:450/2500, reward:3117.28857421875\n",
      "epoch:1, batch:500/2500, reward:3084.28564453125\n",
      "epoch:1, batch:550/2500, reward:3043.06005859375\n",
      "epoch:1, batch:600/2500, reward:3039.11474609375\n",
      "epoch:1, batch:650/2500, reward:3022.846435546875\n",
      "epoch:1, batch:700/2500, reward:2980.724609375\n",
      "epoch:1, batch:750/2500, reward:2989.03173828125\n",
      "epoch:1, batch:800/2500, reward:2865.53173828125\n",
      "epoch:1, batch:850/2500, reward:2821.749267578125\n",
      "epoch:1, batch:900/2500, reward:2848.2861328125\n",
      "epoch:1, batch:950/2500, reward:2804.6142578125\n",
      "epoch:1, batch:1000/2500, reward:2756.9091796875\n",
      "epoch:1, batch:1050/2500, reward:2735.903564453125\n",
      "epoch:1, batch:1100/2500, reward:2734.6484375\n",
      "epoch:1, batch:1150/2500, reward:2681.908447265625\n",
      "epoch:1, batch:1200/2500, reward:2686.30419921875\n",
      "epoch:1, batch:1250/2500, reward:2624.6943359375\n",
      "epoch:1, batch:1300/2500, reward:2660.7431640625\n",
      "epoch:1, batch:1350/2500, reward:2640.6318359375\n",
      "epoch:1, batch:1400/2500, reward:2629.017333984375\n",
      "epoch:1, batch:1450/2500, reward:2576.797119140625\n",
      "epoch:1, batch:1500/2500, reward:2599.599609375\n",
      "epoch:1, batch:1550/2500, reward:2556.46826171875\n",
      "epoch:1, batch:1600/2500, reward:2539.244384765625\n",
      "epoch:1, batch:1650/2500, reward:2583.166015625\n",
      "epoch:1, batch:1700/2500, reward:2478.472900390625\n",
      "epoch:1, batch:1750/2500, reward:2506.08349609375\n",
      "epoch:1, batch:1800/2500, reward:2446.57080078125\n",
      "epoch:1, batch:1850/2500, reward:2464.180908203125\n",
      "epoch:1, batch:1900/2500, reward:2460.33447265625\n",
      "epoch:1, batch:1950/2500, reward:2457.65869140625\n",
      "epoch:1, batch:2000/2500, reward:2425.329345703125\n",
      "epoch:1, batch:2050/2500, reward:2396.01708984375\n",
      "epoch:1, batch:2100/2500, reward:2377.994140625\n",
      "epoch:1, batch:2150/2500, reward:2401.3369140625\n",
      "epoch:1, batch:2200/2500, reward:2377.164306640625\n",
      "epoch:1, batch:2250/2500, reward:2432.060302734375\n",
      "epoch:1, batch:2300/2500, reward:2387.474609375\n",
      "epoch:1, batch:2350/2500, reward:2348.2392578125\n",
      "epoch:1, batch:2400/2500, reward:2374.509521484375\n",
      "epoch:1, batch:2450/2500, reward:2407.272216796875\n",
      "epoch:1, batch:2500/2500, reward:2345.9052734375\n",
      "Avg Actor 2103.54638671875 --- Avg Critic 2764.1279296875\n",
      "My actor is going on the right road Hallelujah :) Updated\n",
      "Epoch: 1, epoch time: 77.039min, tot time: 0.106day, L_actor: 2103.546, L_critic: 2764.128, update: True\n",
      "Save Checkpoints\n",
      "epoch:2, batch:50/2500, reward:2372.82177734375\n",
      "epoch:2, batch:100/2500, reward:2347.6982421875\n",
      "epoch:2, batch:150/2500, reward:2354.178955078125\n",
      "epoch:2, batch:200/2500, reward:2440.0869140625\n",
      "epoch:2, batch:250/2500, reward:2401.475830078125\n",
      "epoch:2, batch:300/2500, reward:2370.52197265625\n",
      "epoch:2, batch:350/2500, reward:2370.34033203125\n",
      "epoch:2, batch:400/2500, reward:2354.3369140625\n",
      "epoch:2, batch:450/2500, reward:2346.106689453125\n",
      "epoch:2, batch:500/2500, reward:2311.330322265625\n",
      "epoch:2, batch:550/2500, reward:2259.760986328125\n",
      "epoch:2, batch:600/2500, reward:2271.6572265625\n",
      "epoch:2, batch:650/2500, reward:2278.9990234375\n",
      "epoch:2, batch:700/2500, reward:2276.50390625\n",
      "epoch:2, batch:750/2500, reward:2267.73974609375\n",
      "epoch:2, batch:800/2500, reward:2250.81689453125\n",
      "epoch:2, batch:850/2500, reward:2250.67138671875\n",
      "epoch:2, batch:900/2500, reward:2201.96142578125\n",
      "epoch:2, batch:950/2500, reward:2203.43603515625\n",
      "epoch:2, batch:1000/2500, reward:2224.1142578125\n",
      "epoch:2, batch:1050/2500, reward:2266.7236328125\n",
      "epoch:2, batch:1100/2500, reward:2221.65771484375\n",
      "epoch:2, batch:1150/2500, reward:2235.2333984375\n",
      "epoch:2, batch:1200/2500, reward:2203.946533203125\n",
      "epoch:2, batch:1250/2500, reward:2216.4453125\n",
      "epoch:2, batch:1300/2500, reward:2208.3037109375\n",
      "epoch:2, batch:1350/2500, reward:2230.420654296875\n",
      "epoch:2, batch:1400/2500, reward:2180.840087890625\n",
      "epoch:2, batch:1450/2500, reward:2217.918701171875\n",
      "epoch:2, batch:1500/2500, reward:2200.99169921875\n",
      "epoch:2, batch:1550/2500, reward:2220.8232421875\n",
      "epoch:2, batch:1600/2500, reward:2171.227783203125\n",
      "epoch:2, batch:1650/2500, reward:2188.974853515625\n",
      "epoch:2, batch:1700/2500, reward:2206.134521484375\n",
      "epoch:2, batch:1750/2500, reward:2219.5185546875\n",
      "epoch:2, batch:1800/2500, reward:2202.2646484375\n",
      "epoch:2, batch:1850/2500, reward:2217.498046875\n",
      "epoch:2, batch:1900/2500, reward:2178.83203125\n",
      "epoch:2, batch:1950/2500, reward:2175.55224609375\n",
      "epoch:2, batch:2000/2500, reward:2198.550048828125\n",
      "epoch:2, batch:2050/2500, reward:2174.576416015625\n",
      "epoch:2, batch:2100/2500, reward:2175.371337890625\n",
      "epoch:2, batch:2150/2500, reward:2184.975341796875\n",
      "epoch:2, batch:2200/2500, reward:2164.977783203125\n",
      "epoch:2, batch:2250/2500, reward:2174.703369140625\n",
      "epoch:2, batch:2300/2500, reward:2164.26025390625\n",
      "epoch:2, batch:2350/2500, reward:2175.0009765625\n",
      "epoch:2, batch:2400/2500, reward:2203.163330078125\n",
      "epoch:2, batch:2450/2500, reward:2153.846923828125\n",
      "epoch:2, batch:2500/2500, reward:2186.765625\n",
      "Avg Actor 2047.848388671875 --- Avg Critic 2103.54638671875\n",
      "My actor is going on the right road Hallelujah :) Updated\n",
      "Epoch: 2, epoch time: 77.781min, tot time: 0.160day, L_actor: 2047.848, L_critic: 2103.546, update: True\n",
      "Save Checkpoints\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:3, batch:50/2500, reward:2184.077392578125\n",
      "epoch:3, batch:100/2500, reward:2195.165283203125\n",
      "epoch:3, batch:150/2500, reward:2162.1025390625\n",
      "epoch:3, batch:200/2500, reward:2167.10498046875\n",
      "epoch:3, batch:250/2500, reward:2181.922607421875\n",
      "epoch:3, batch:300/2500, reward:2203.193359375\n",
      "epoch:3, batch:350/2500, reward:2176.8740234375\n",
      "epoch:3, batch:400/2500, reward:2320.352294921875\n",
      "epoch:3, batch:450/2500, reward:2280.897216796875\n",
      "epoch:3, batch:500/2500, reward:2205.53955078125\n",
      "epoch:3, batch:550/2500, reward:2154.4765625\n",
      "epoch:3, batch:600/2500, reward:2192.24755859375\n",
      "epoch:3, batch:650/2500, reward:2169.682861328125\n",
      "epoch:3, batch:700/2500, reward:2176.54296875\n",
      "epoch:3, batch:750/2500, reward:2148.230224609375\n",
      "epoch:3, batch:800/2500, reward:2162.438232421875\n",
      "epoch:3, batch:850/2500, reward:2153.72509765625\n",
      "epoch:3, batch:900/2500, reward:2179.81494140625\n",
      "epoch:3, batch:950/2500, reward:2180.64404296875\n",
      "epoch:3, batch:1000/2500, reward:2166.634521484375\n",
      "epoch:3, batch:1050/2500, reward:2177.23828125\n",
      "epoch:3, batch:1100/2500, reward:2153.81884765625\n",
      "epoch:3, batch:1150/2500, reward:2183.96826171875\n",
      "epoch:3, batch:1200/2500, reward:2172.12353515625\n",
      "epoch:3, batch:1250/2500, reward:2155.8349609375\n",
      "epoch:3, batch:1300/2500, reward:2146.762451171875\n",
      "epoch:3, batch:1350/2500, reward:2147.841064453125\n",
      "epoch:3, batch:1400/2500, reward:2135.836669921875\n",
      "epoch:3, batch:1450/2500, reward:2156.010009765625\n",
      "epoch:3, batch:1500/2500, reward:2128.8125\n",
      "epoch:3, batch:1550/2500, reward:2147.00244140625\n",
      "epoch:3, batch:1600/2500, reward:2131.982666015625\n",
      "epoch:3, batch:1650/2500, reward:2156.701171875\n",
      "epoch:3, batch:1700/2500, reward:2128.844970703125\n",
      "epoch:3, batch:1750/2500, reward:2138.84130859375\n",
      "epoch:3, batch:1800/2500, reward:2155.940185546875\n",
      "epoch:3, batch:1850/2500, reward:2116.84130859375\n",
      "epoch:3, batch:1900/2500, reward:2152.2880859375\n",
      "epoch:3, batch:1950/2500, reward:2133.146484375\n"
     ]
    }
   ],
   "source": [
    "from torch.distributions.categorical import Categorical\n",
    "# visualization\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "from tqdm import tqdm_notebook\n",
    "####### my own import file ##########\n",
    "from img_index import gazebo_showup\n",
    "from listofpathpoint import input_handler\n",
    "import cnc_input\n",
    "#from hybrid_models import HPN\n",
    "####### my own import file ##########\n",
    "'''\n",
    "so, the models we have are TransEncoderNet,\n",
    "                            Attention\n",
    "                            LSTM\n",
    "                            HPN\n",
    "each one have initial parameters and the forward part, \n",
    "once we have the forward part, the back propagation will \n",
    "finished automatically by pytorch  \n",
    "'''\n",
    "size = 136\n",
    "TOL = 1e-3\n",
    "TINY = 1e-15\n",
    "learning_rate = 1e-4   #learning rate\n",
    "B = 256             #batch size\n",
    "B_valLoop = 20\n",
    "steps = 2500\n",
    "n_epoch = 100       # epochs\n",
    "size_rec = int(size/4)\n",
    "\n",
    "print('======================')\n",
    "print('prepare to train')\n",
    "print('======================')\n",
    "print('Hyper parameters:')\n",
    "print('learning rate', learning_rate)\n",
    "print('batch size', B)\n",
    "print('steps', steps)\n",
    "print('epoch', n_epoch)\n",
    "print('======================')\n",
    "\n",
    "'''\n",
    "instantiate a training network and a baseline network\n",
    "'''\n",
    "temp = input_handler('mother_board.json')\n",
    "'''\n",
    "X_val consisted by 'list of list of list'\n",
    "'rectangle list' 'channel list' 'point xy list' respectively\n",
    "'''\n",
    "try:\n",
    "    del Actor  # remove existing model\n",
    "    del Critic # remove existing model\n",
    "except:\n",
    "    pass\n",
    "Actor = HPN(n_feature = 2, n_hidden = 128)\n",
    "Critic = HPN(n_feature = 2, n_hidden = 128)\n",
    "optimizer = optim.Adam(Actor.parameters(), lr=learning_rate)\n",
    "\n",
    "# Putting Critic model on the eval mode\n",
    "Actor = Actor.to(device)\n",
    "Critic = Critic.to(device)\n",
    "Critic.eval()\n",
    "\n",
    "epoch_ckpt = 0\n",
    "tot_time_ckpt = 0\n",
    "\n",
    "val_mean = []\n",
    "val_std = []\n",
    "\n",
    "plot_performance_train = []\n",
    "plot_performance_baseline = []\n",
    "# recording the result of the resent epoch makes it available for future\n",
    "#*********************# Uncomment these lines to load the previous check point\n",
    "\"\"\"\n",
    "checkpoint_file = \"checkpoint/checkpoint_21-11-28--19-13-20-n136-gpu0.pkl\"\n",
    "checkpoint = torch.load(checkpoint_file, map_location=device)\n",
    "epoch_ckpt = checkpoint['epoch'] + 1\n",
    "tot_time_ckpt = checkpoint['tot_time']\n",
    "plot_performance_train = checkpoint['plot_performance_train']\n",
    "plot_performance_baseline = checkpoint['plot_performance_baseline']\n",
    "Critic.load_state_dict(checkpoint['model_baseline'])\n",
    "Actor.load_state_dict(checkpoint['model_train'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "\n",
    "print('Re-start training with saved checkpoint file={:s}\\n  Checkpoint at epoch= {:d} and time={:.3f}min\\n'.format(checkpoint_file,epoch_ckpt-1,tot_time_ckpt/60))\n",
    "\n",
    "\"\"\"\n",
    "#***********************# Uncomment these lines to load the previous check point\n",
    "\n",
    "# Main training loop\n",
    "# The core training concept mainly upon Sampling from the actor\n",
    "# then taking the greedy action from the critic\n",
    "\n",
    "\n",
    "start_training_time = time.time()\n",
    "time_stamp = datetime.datetime.now().strftime(\"%y-%m-%d--%H-%M-%S\") # Load the time stamp\n",
    "\n",
    "C = 0       # baseline => the object which the actor can compare\n",
    "R = 0       # reward\n",
    "\n",
    "zero_to_bsz = torch.arange(B, device = device) # a list contains 0 to (batch size -1)\n",
    "\n",
    "for epoch in range(0, n_epoch):\n",
    "    \n",
    "    # re-start training with saved checkpoint\n",
    "    epoch += epoch_ckpt # adding the number of the former epochs\n",
    "    # Train the model for one epoch\n",
    "    start = time.time() # record the starting time\n",
    "    Actor.train() \n",
    "    X_temp = temp.every_point()\n",
    "    X_temp = torch.FloatTensor(X_temp)\n",
    "    f_temp = input_handler('mother_board.json')\n",
    "    X = X_temp.repeat(B,1,1)\n",
    "    X = X.cuda()\n",
    "    path_gazebo = []\n",
    "    \n",
    "    for i in range(1, steps+1): # 1 ~ 2500 steps\n",
    "        mask = torch.zeros(B,size).cuda() # use mask to make some points impossible to choose\n",
    "        R = 0\n",
    "        logprobs = 0\n",
    "        reward = 0\n",
    "        Y = X.view(B,size,2)\n",
    "        x = torch.zeros(B,2)#Y[:,0,:] #set the first point to x\n",
    "        h = None\n",
    "        c = None\n",
    "        context = None        #set Y_ini to the out corner\n",
    "        Transcontext = None\n",
    "        Y0 = None\n",
    "        # Actor Sampling phase\n",
    "        for k in range(size_rec):\n",
    "            context, Transcontext, output, h, c, _ = Actor(context,Transcontext,x=x, X_all=X, h=h, c=c, mask=mask)\n",
    "            sampler = torch.distributions.Categorical(output)\n",
    "            idx = sampler.sample()\n",
    "             #prepare for the back propagation of pytorch\n",
    "            reward, Y0,x = rectangle_process(f_temp, idx,Y,Y0, mask,k,B,i,path_gazebo)\n",
    "            R += reward\n",
    "            logprobs += torch.log(output[zero_to_bsz, idx.data] + TINY)\n",
    "            ##mask[zero_to_bsz, idx.data] += -np.inf\n",
    "# critic baseline phase, use the baseline to compute the actual reward of agent at that time\n",
    "        mask = torch.zeros(B,size).cuda() # use mask to make some points impossible to choose\n",
    "        C = 0\n",
    "        baseline = 0\n",
    "        Y = X.view(B,size,2)\n",
    "        x = torch.zeros(B,2)#Y[:,0,:] #set the first point to x\n",
    "        h = None\n",
    "        c = None\n",
    "        context = None\n",
    "        Transcontext = None\n",
    "        C0 = None\n",
    "        # compute tours for baseline without grad \"Cause we want to fix the weights for the critic\"\n",
    "        with torch.no_grad():\n",
    "            for k in range(size_rec):      \n",
    "                context, Transcontext, output, h, c, _ = Critic(context,Transcontext,x=x, X_all=X, h=h, c=c, mask=mask)\n",
    "                idx = torch.argmax(output, dim=1) # ----> greedy baseline critic\n",
    "                # prepare for the back propagation of pytorch\n",
    "                baseline, C0,x = rectangle_process(f_temp,idx,Y,C0, mask,k,B)\n",
    "                C += baseline\n",
    "        ###################\n",
    "        # Loss and backprop handling \n",
    "        ###################\n",
    "        \n",
    "        loss = torch.mean((R - C) * logprobs)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if i % 50 == 0:\n",
    "            print(\"epoch:{}, batch:{}/{}, reward:{}\".format(epoch, i, steps, R.mean().item()))\n",
    "        if i % 200 == 0:\n",
    "            print(\"record the last path to gazebo for showing up\")\n",
    "            #starting to show the path on simulated enviroment of cnc_machine \n",
    "            the_resent_path = temp.zig_zag_path(path_gazebo)\n",
    "            gazebo_showup(the_resent_path)\n",
    "    time_one_epoch = time.time() - start #recording the work time of one epoch\n",
    "    time_tot = time.time() - start_training_time + tot_time_ckpt\n",
    "    ###################\n",
    "    # Evaluate train model and baseline \n",
    "    # in this phase we just solve random instances with the actor and the critic\n",
    "    # compare this soluation if we get any improvment we'll transfer the actor's\n",
    "    # weights into the critic\n",
    "    ###################\n",
    "    # putting the actor in the eval mode\n",
    "    Actor.eval()\n",
    "    \n",
    "    mean_tour_length_actor = 0\n",
    "    mean_tour_length_critic = 0\n",
    "\n",
    "    for step in range(0,B_valLoop):\n",
    "        \n",
    "        # compute tour for model and baseline\n",
    "        X_temp_val = temp.every_point()\n",
    "        X_temp_val = torch.FloatTensor(X_temp_val)\n",
    "        X_val = X_temp_val.repeat(B,1,1)\n",
    "        X_val = X_val.cuda()\n",
    "        mask = torch.zeros(B,size).cuda()\n",
    "        R = 0\n",
    "        reward = 0\n",
    "        Y = X_val.view(B,size,2)\n",
    "        x = torch.zeros(B,2)#Y[:,0,:] #set the first point to x\n",
    "        Y0 = None\n",
    "        h = None\n",
    "        c = None\n",
    "        context = None\n",
    "        Transcontext = None\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for k in range(size_rec):\n",
    "                #same as the above part\n",
    "                context, Transcontext, output, h, c, _ = Actor(context,Transcontext,x=x, X_all=X, h=h, c=c, mask=mask)\n",
    "                idx = torch.argmax(output, dim=1)\n",
    "                # prepare for the back propagation of pytorch\n",
    "                reward, Y0,x = rectangle_process(f_temp, idx,Y,Y0, mask,k,B)\n",
    "                R += reward\n",
    "        # critic baseline\n",
    "        mask = torch.zeros(B,size).cuda()\n",
    "        C = 0\n",
    "        baseline = 0\n",
    "        \n",
    "        Y = X_val.view(B,size,2)\n",
    "        x = torch.zeros(B,2)#Y[:,0,:] #set the first point to x\n",
    "        \n",
    "        h = None\n",
    "        c = None\n",
    "        context = None\n",
    "        Transcontext = None\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for k in range(size_rec):\n",
    "                #same as the above part\n",
    "                context, Transcontext, output, h, c, _ = Critic(context,Transcontext,x=x, X_all=X, h=h, c=c, mask=mask)\n",
    "                idx = torch.argmax(output, dim=1)  \n",
    "                # prepare for the back propagation of pytorch\n",
    "                baseline, Y0,x = rectangle_process(f_temp, idx,Y,Y0, mask,k,B)\n",
    "                C += baseline\n",
    "\n",
    "        mean_tour_length_actor  += R.mean().item()\n",
    "        mean_tour_length_critic += C.mean().item()\n",
    "\n",
    "    mean_tour_length_actor  =  mean_tour_length_actor  / B_valLoop\n",
    "    mean_tour_length_critic =  mean_tour_length_critic / B_valLoop\n",
    "    # evaluate train model and baseline and update if train model is better\n",
    "\n",
    "    update_baseline = mean_tour_length_actor + TOL < mean_tour_length_critic\n",
    "\n",
    "    print('Avg Actor {} --- Avg Critic {}'.format(mean_tour_length_actor,mean_tour_length_critic))\n",
    "\n",
    "    if update_baseline:\n",
    "        Critic.load_state_dict(Actor.state_dict())\n",
    "        print('My actor is going on the right road Hallelujah :) Updated')\n",
    "    ###################\n",
    "    # Valdiation train model and baseline on 1k random TSP instances\n",
    "    ###################\n",
    "    # erased by daniel due to the 1K tsp is not the scale I want to train  \n",
    "\n",
    "    # For checkpoint\n",
    "    plot_performance_train.append([(epoch+1), mean_tour_length_actor])\n",
    "    plot_performance_baseline.append([(epoch+1), mean_tour_length_critic])\n",
    "    # compute the optimally gap ==> this is interesting because there is no LKH or other optimal algorithms \n",
    "    # for the problem like this rectangle characterized map\n",
    "    mystring_min = 'Epoch: {:d}, epoch time: {:.3f}min, tot time: {:.3f}day, L_actor: {:.3f}, L_critic: {:.3f}, update: {}'.format(\n",
    "        epoch, time_one_epoch/60, time_tot/86400, mean_tour_length_actor, mean_tour_length_critic, update_baseline)\n",
    "\n",
    "    print(mystring_min)\n",
    "    print('Save Checkpoints')\n",
    "\n",
    "    # Saving checkpoint\n",
    "    checkpoint_dir = os.path.join(\"checkpoint\")\n",
    "\n",
    "    if not os.path.exists(checkpoint_dir):\n",
    "        os.makedirs(checkpoint_dir)\n",
    "    \n",
    "    torch.save({\n",
    "        'epoch': epoch,\n",
    "        'time': time_one_epoch,\n",
    "        'tot_time': time_tot,\n",
    "        'loss': loss.item(),\n",
    "        'plot_performance_train': plot_performance_train,\n",
    "        'plot_performance_baseline': plot_performance_baseline,\n",
    "        'model_baseline': Critic.state_dict(),\n",
    "        'model_train': Actor.state_dict(),\n",
    "        'optimizer': optimizer.state_dict(),\n",
    "        },'{}.pkl'.format(checkpoint_dir + \"/checkpoint_\" + time_stamp + \"-n{}\".format(size) + \"-gpu{}\".format(gpu_id)))\n",
    "\n",
    "\n",
    "\n",
    "  \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                \n",
    "            \n",
    "                \n",
    "        \n",
    "                        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "index = int(9.34)\n",
    "table = [[index + 1,index + 2,index + 3],[index + 4,index + 5,index + 6],[index + 7,index + 8,index + 9]]\n",
    "gll = torch.as_tensor(table)\n",
    "i = gll[:,0]\n",
    "print(i)\n",
    "for j in gll:\n",
    "    print(j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "a = torch.tensor([])\n",
    "b = torch.Tensor([[4, 5, 6]])\n",
    "c = torch.tensor([[7, 8, 9]])\n",
    "d =  torch.cat((a, b, c), 0)\n",
    "print(d.shape)\n",
    "e =  torch.cat((a, b, c), 1)\n",
    "print(e.shape)\n",
    "i = d[:,0]\n",
    "print(i)\n",
    "print('0:', torch.cat((torch.Tensor([[4, 5, 6]]), d), 0))\n",
    "print(torch.cat((b, d),0).shape)\n",
    "print('1:', torch.cat((a, b, c), 1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
