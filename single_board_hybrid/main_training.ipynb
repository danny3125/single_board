{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU name: GeForce RTX 2080 Ti, gpu_id: 0\n",
      "cuda\n",
      "pytorch version =  1.10.0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import time\n",
    "import argparse\n",
    "\n",
    "import os\n",
    "import datetime\n",
    "\n",
    "from torch.distributions.categorical import Categorical\n",
    "\n",
    "# visualization \n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "device = torch.device(\"cpu\"); gpu_id = -1 # select CPU\n",
    "\n",
    "gpu_id = '0' # select a single GPU  \n",
    "#gpu_id = '2,3' # select multiple GPUs  \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(gpu_id)  \n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print('GPU name: {:s}, gpu_id: {:s}'.format(torch.cuda.get_device_name(0),gpu_id))   \n",
    "    \n",
    "print(device)\n",
    "print('pytorch version = ',torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from torch.optim import lr_scheduler\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm_notebook\n",
    "class TransEncoderNet(nn.Module):\n",
    "    \"\"\"\n",
    "    Encoder network based on self-attention transformer\n",
    "    Inputs :  \n",
    "      h of size      (bsz, nb_nodes, dim_emb)    batch of input cities\n",
    "    Outputs :  \n",
    "      h of size      (bsz, nb_nodes, dim_emb)    batch of encoded cities\n",
    "      score of size  (bsz, nb_nodes, nb_nodes+1) batch of attention scores\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, nb_layers, dim_emb, nb_heads, dim_ff, batchnorm):\n",
    "        super(TransEncoderNet, self).__init__()\n",
    "        assert dim_emb == nb_heads* (dim_emb//nb_heads) # check if dim_emb is divisible by nb_heads\n",
    "        self.MHA_layers = nn.ModuleList( [nn.MultiheadAttention(dim_emb, nb_heads) for _ in range(nb_layers)] )\n",
    "        self.linear1_layers = nn.ModuleList( [nn.Linear(dim_emb, dim_ff) for _ in range(nb_layers)] )\n",
    "        self.linear2_layers = nn.ModuleList( [nn.Linear(dim_ff, dim_emb) for _ in range(nb_layers)] )   \n",
    "        if batchnorm:\n",
    "            self.norm1_layers = nn.ModuleList( [nn.BatchNorm1d(dim_emb) for _ in range(nb_layers)] )\n",
    "            self.norm2_layers = nn.ModuleList( [nn.BatchNorm1d(dim_emb) for _ in range(nb_layers)] )\n",
    "        else:\n",
    "            self.norm1_layers = nn.ModuleList( [nn.LayerNorm(dim_emb) for _ in range(nb_layers)] )\n",
    "            self.norm2_layers = nn.ModuleList( [nn.LayerNorm(dim_emb) for _ in range(nb_layers)] )\n",
    "        self.nb_layers = nb_layers\n",
    "        self.nb_heads = nb_heads\n",
    "        self.batchnorm = batchnorm\n",
    "        \n",
    "    def forward(self, h):      \n",
    "        # PyTorch nn.MultiheadAttention requires input size (seq_len, bsz, dim_emb) \n",
    "        h = h.transpose(0,1) # size(h)=(nb_nodes, bsz, dim_emb)  \n",
    "        # L layers\n",
    "        for i in range(self.nb_layers):\n",
    "            h_rc = h # residual connection, size(h_rc)=(nb_nodes, bsz, dim_emb)\n",
    "            h, score = self.MHA_layers[i](h, h, h) # size(h)=(nb_nodes, bsz, dim_emb), size(score)=(bsz, nb_nodes, nb_nodes)\n",
    "            # add residual connection\n",
    "            \n",
    "            h = h_rc + h # size(h)=(nb_nodes, bsz, dim_emb)\n",
    "            if self.batchnorm:\n",
    "                # Pytorch nn.BatchNorm1d requires input size (bsz, dim, seq_len)\n",
    "                h = h.permute(1,2,0).contiguous() # size(h)=(bsz, dim_emb, nb_nodes)\n",
    "                h = self.norm1_layers[i](h)       # size(h)=(bsz, dim_emb, nb_nodes)\n",
    "                h = h.permute(2,0,1).contiguous() # size(h)=(nb_nodes, bsz, dim_emb)\n",
    "            else:\n",
    "                h = self.norm1_layers[i](h)       # size(h)=(nb_nodes, bsz, dim_emb) \n",
    "            # feedforward\n",
    "            h_rc = h # residual connection\n",
    "            h = self.linear2_layers[i](torch.relu(self.linear1_layers[i](h)))\n",
    "            h = h_rc + h # size(h)=(nb_nodes, bsz, dim_emb)\n",
    "            if self.batchnorm:\n",
    "                h = h.permute(1,2,0).contiguous() # size(h)=(bsz, dim_emb, nb_nodes)\n",
    "                h = self.norm2_layers[i](h)       # size(h)=(bsz, dim_emb, nb_nodes)\n",
    "                h = h.permute(2,0,1).contiguous() # size(h)=(nb_nodes, bsz, dim_emb)\n",
    "            else:\n",
    "                h = self.norm2_layers[i](h) # size(h)=(nb_nodes, bsz, dim_emb)\n",
    "        # Transpose h\n",
    "        h = h.transpose(0,1) # size(h)=(bsz, nb_nodes, dim_emb)\n",
    "        return h, score\n",
    "    \n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, n_hidden):\n",
    "        super(Attention, self).__init__()\n",
    "        self.size = 0\n",
    "        self.batch_size = 0\n",
    "        self.dim = n_hidden\n",
    "        \n",
    "        v  = torch.FloatTensor(n_hidden)\n",
    "        self.v  = nn.Parameter(v)\n",
    "        self.v.data.uniform_(-1/math.sqrt(n_hidden), 1/math.sqrt(n_hidden))\n",
    "        \n",
    "        # parameters for pointer attention\n",
    "        self.Wref = nn.Linear(n_hidden, n_hidden)\n",
    "        self.Wq = nn.Linear(n_hidden, n_hidden)\n",
    "    \n",
    "    \n",
    "    def forward(self, q, ref):       # query and reference\n",
    "        self.batch_size = q.size(0)\n",
    "        self.size = int(ref.size(0) / self.batch_size)\n",
    "        q = self.Wq(q)     # (B, dim)\n",
    "        ref = self.Wref(ref)\n",
    "        ref = ref.view(self.batch_size, self.size, self.dim)  # (B, size, dim)\n",
    "        \n",
    "        q_ex = q.unsqueeze(1).repeat(1, self.size, 1) # (B, size, dim)\n",
    "        # v_view: (B, dim, 1)\n",
    "        v_view = self.v.unsqueeze(0).expand(self.batch_size, self.dim).unsqueeze(2)\n",
    "        \n",
    "        # (B, size, dim) * (B, dim, 1)\n",
    "        u = torch.bmm(torch.tanh(q_ex + ref), v_view).squeeze(2)\n",
    "        \n",
    "        return u, ref\n",
    "    \n",
    "class LSTM(nn.Module):\n",
    "    def __init__(self, n_hidden):\n",
    "        super(LSTM, self).__init__()\n",
    "        \n",
    "        # parameters for input gate\n",
    "        self.Wxi = nn.Linear(n_hidden, n_hidden)    # W(xt)\n",
    "        self.Whi = nn.Linear(n_hidden, n_hidden)    # W(ht)\n",
    "        self.wci = nn.Linear(n_hidden, n_hidden)    # w(ct)\n",
    "        \n",
    "        # parameters for forget gate\n",
    "        self.Wxf = nn.Linear(n_hidden, n_hidden)    # W(xt)\n",
    "        self.Whf = nn.Linear(n_hidden, n_hidden)    # W(ht)\n",
    "        self.wcf = nn.Linear(n_hidden, n_hidden)    # w(ct)\n",
    "        \n",
    "        # parameters for cell gate\n",
    "        self.Wxc = nn.Linear(n_hidden, n_hidden)    # W(xt)\n",
    "        self.Whc = nn.Linear(n_hidden, n_hidden)    # W(ht)\n",
    "        \n",
    "        # parameters for forget gate\n",
    "        self.Wxo = nn.Linear(n_hidden, n_hidden)    # W(xt)\n",
    "        self.Who = nn.Linear(n_hidden, n_hidden)    # W(ht)\n",
    "        self.wco = nn.Linear(n_hidden, n_hidden)    # w(ct)\n",
    "    \n",
    "    \n",
    "    def forward(self, x, h, c):       # query and reference\n",
    "        \n",
    "        # input gate\n",
    "        i = torch.sigmoid(self.Wxi(x) + self.Whi(h) + self.wci(c))\n",
    "        # forget gate\n",
    "        f = torch.sigmoid(self.Wxf(x) + self.Whf(h) + self.wcf(c))\n",
    "        # cell gate\n",
    "        c = f * c + i * torch.tanh(self.Wxc(x) + self.Whc(h))\n",
    "        # output gate\n",
    "        o = torch.sigmoid(self.Wxo(x) + self.Who(h) + self.wco(c))\n",
    "        \n",
    "        h = o * torch.tanh(c)\n",
    "        \n",
    "        return h, c\n",
    "\n",
    "class HPN(nn.Module):\n",
    "    def __init__(self, n_feature, n_hidden):\n",
    "\n",
    "        super(HPN, self).__init__()\n",
    "        self.city_size = 0\n",
    "        self.batch_size = 0\n",
    "        self.dim = n_hidden\n",
    "        \n",
    "        # lstm for first turn\n",
    "        #self.lstm0 = nn.LSTM(n_hidden, n_hidden)\n",
    "        \n",
    "        # pointer layer\n",
    "        self.pointer = Attention(n_hidden)\n",
    "        self.TransPointer = Attention(n_hidden)\n",
    "        \n",
    "        # lstm encoder\n",
    "        self.encoder = LSTM(n_hidden)\n",
    "        \n",
    "        # trainable first hidden input\n",
    "        h0 = torch.FloatTensor(n_hidden)\n",
    "        c0 = torch.FloatTensor(n_hidden)\n",
    "        # trainable latent variable coefficient\n",
    "        print('here') \n",
    "        alpha = torch.ones(1).cuda()       \n",
    "        self.h0 = nn.Parameter(h0)\n",
    "        self.c0 = nn.Parameter(c0)\n",
    "        \n",
    "        self.alpha = nn.Parameter(alpha)\n",
    "        self.h0.data.uniform_(-1/math.sqrt(n_hidden), 1/math.sqrt(n_hidden))\n",
    "        self.c0.data.uniform_(-1/math.sqrt(n_hidden), 1/math.sqrt(n_hidden))\n",
    "        \n",
    "        r1 = torch.ones(1)\n",
    "        r2 = torch.ones(1)\n",
    "        r3 = torch.ones(1)\n",
    "        self.r1 = nn.Parameter(r1)\n",
    "        self.r2 = nn.Parameter(r2)\n",
    "        self.r3 = nn.Parameter(r3)\n",
    "        \n",
    "        # embedding\n",
    "        self.embedding_x = nn.Linear(n_feature, n_hidden)\n",
    "        self.embedding_all = nn.Linear(n_feature, n_hidden)\n",
    "        self.Transembedding_all = TransEncoderNet(6, 128, 8, 512, batchnorm=True)#6,128,8,512\n",
    "        \n",
    "        # vector to start decoding \n",
    "        self.start_placeholder = nn.Parameter(torch.randn(n_hidden))\n",
    "        \n",
    "        # weights for GNN\n",
    "        self.W1 = nn.Linear(n_hidden, n_hidden)\n",
    "        self.W2 = nn.Linear(n_hidden, n_hidden)\n",
    "        self.W3 = nn.Linear(n_hidden, n_hidden)\n",
    "        \n",
    "        # aggregation function for GNN\n",
    "        self.agg_1 = nn.Linear(n_hidden, n_hidden)\n",
    "        self.agg_2 = nn.Linear(n_hidden, n_hidden)\n",
    "        self.agg_3 = nn.Linear(n_hidden, n_hidden)\n",
    "    \n",
    "    \n",
    "    def forward(self,context,Transcontext, x, X_all, mask, h=None, c=None, latent=None):\n",
    "        '''\n",
    "        Inputs (B: batch size, size: city size, dim: hidden dimension)\n",
    "        \n",
    "        x: current city coordinate (B, 2)\n",
    "        X_all: all cities' cooridnates (B, size, 2)\n",
    "        mask: mask visited cities\n",
    "        h: hidden variable (B, dim)\n",
    "        c: cell gate (B, dim)\n",
    "        latent: latent pointer vector from previous layer (B, size, dim)\n",
    "        \n",
    "        Outputs\n",
    "        \n",
    "        softmax: probability distribution of next city (B, size)\n",
    "        h: hidden variable (B, dim)\n",
    "        c: cell gate (B, dim)\n",
    "        latent_u: latent pointer vector for next layer\n",
    "        '''\n",
    "        \n",
    "        self.batch_size = X_all.size(0)\n",
    "        self.city_size = X_all.size(1)\n",
    "        \n",
    "        # Check if this the first iteration loop\n",
    "        if h is None or c is None:\n",
    "            x          = self.start_placeholder    \n",
    "            context = self.embedding_all(X_all)\n",
    "            Transcontext,_ = self.Transembedding_all(context)\n",
    "            \n",
    "            # =============================\n",
    "            # graph neural network encoder\n",
    "            # =============================\n",
    "\n",
    "            # (B, size, dim)\n",
    "            context = context.reshape(-1, self.dim)\n",
    "            Transcontext = Transcontext.reshape(-1, self.dim)\n",
    "\n",
    "            context = self.r1 * self.W1(context)\\\n",
    "                + (1-self.r1) * F.relu(self.agg_1(context/(self.city_size-1)))\n",
    "\n",
    "            context = self.r2 * self.W2(context)\\\n",
    "                + (1-self.r2) * F.relu(self.agg_2(context/(self.city_size-1)))\n",
    "\n",
    "            context = self.r3 * self.W3(context)\\\n",
    "                + (1-self.r3) * F.relu(self.agg_3(context/(self.city_size-1)))\n",
    "            h0 = self.h0.unsqueeze(0).expand(self.batch_size, self.dim)\n",
    "            c0 = self.c0.unsqueeze(0).expand(self.batch_size, self.dim)\n",
    "\n",
    "            h0 = h0.unsqueeze(0).contiguous()\n",
    "            c0 = c0.unsqueeze(0).contiguous()\n",
    "            \n",
    "            # let h0, c0 be the hidden variable of first turn\n",
    "            h = h0.squeeze(0)\n",
    "            c = c0.squeeze(0)\n",
    "        else:\n",
    "            x          = self.embedding_x(x)\n",
    "        # LSTM encoder\n",
    "        h, c = self.encoder(x, h, c)\n",
    "        # query vector\n",
    "        q = h\n",
    "        # pointer\n",
    "        u1, _ = self.pointer(q, context)\n",
    "        u2 ,_ = self.TransPointer(q,Transcontext)\n",
    "        # Avg Agg between the two attention vectors\n",
    "        u = torch.maximum(u1,u2)\n",
    "        latent_u = u.clone()\n",
    "        u = 10 * torch.tanh(u) + mask\n",
    "        return context,Transcontext,F.softmax(u, dim=1), h, c, latent_u"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# daniel rectangle feature handle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "'''\n",
    "This part I designed the rectangle-characterized TSP, that means for every step the agent walk through a corner,\n",
    "then he travel through the whole rectangle using zig-zag, finally he ends up at one of the rest corners of \n",
    "the rextangle, so, it equals the agent walk through three points at one step, in practice, I add three points into \n",
    "mask to make them unselectable.\n",
    "'''\n",
    "def rectangle_process(f_temp,idx,Y,Y0,mask,k,B):\n",
    "    Y1 = Y[zero_to_bsz, idx.data].clone()\n",
    "    rectangle_inf = idx/4\n",
    "    feature_table = f_temp.outcorner_getout(rectangle_inf,B)\n",
    "    feature_table = torch.Tensor(feature_table).type(torch.long)\n",
    "    Y_corner = Y[zero_to_bsz, idx.data].clone()\n",
    "    if k ==0:\n",
    "        reward = 0\n",
    "    if k > 0:\n",
    "        reward = torch.sum((Y1 - Y0) ** 2, dim=1) ** 0.5\n",
    "        reward += torch.sum((Y_corner - Y1) ** 2, dim=1) ** 0.5   \n",
    "    Y0 = Y[zero_to_bsz, feature_table[:,0].data].clone()\n",
    "    mask[zero_to_bsz, idx.data] += -np.inf\n",
    "    mask[zero_to_bsz, feature_table[:,0].data] += -np.inf\n",
    "    mask[zero_to_bsz, feature_table[:,1].data ] += -np.inf    \n",
    "    mask[zero_to_bsz, feature_table[:,2].data ] += -np.inf \n",
    "    return reward, Y0, Y_corner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================\n",
      "prepare to train\n",
      "======================\n",
      "Hyper parameters:\n",
      "learning rate 0.0001\n",
      "batch size 256\n",
      "steps 2500\n",
      "epoch 100\n",
      "======================\n",
      "here\n",
      "here\n",
      "Re-start training with saved checkpoint file=checkpoint/checkpoint_21-11-28--19-13-20-n136-gpu0.pkl\n",
      "  Checkpoint at epoch= 10 and time=831.430min\n",
      "\n",
      "epoch:11, batch:50/2500, reward:1880.40185546875\n",
      "epoch:11, batch:100/2500, reward:1879.890869140625\n",
      "epoch:11, batch:150/2500, reward:1859.23046875\n",
      "epoch:11, batch:200/2500, reward:1867.44384765625\n",
      "epoch:11, batch:250/2500, reward:1866.350341796875\n",
      "epoch:11, batch:300/2500, reward:1859.5709228515625\n",
      "epoch:11, batch:350/2500, reward:1937.1209716796875\n",
      "epoch:11, batch:400/2500, reward:1893.7542724609375\n",
      "epoch:11, batch:450/2500, reward:1903.85400390625\n",
      "epoch:11, batch:500/2500, reward:1872.923583984375\n",
      "epoch:11, batch:550/2500, reward:1994.6541748046875\n",
      "epoch:11, batch:600/2500, reward:1891.48681640625\n",
      "epoch:11, batch:650/2500, reward:1873.334716796875\n",
      "epoch:11, batch:700/2500, reward:1862.8504638671875\n",
      "epoch:11, batch:750/2500, reward:1865.619873046875\n",
      "epoch:11, batch:800/2500, reward:1865.7420654296875\n",
      "epoch:11, batch:850/2500, reward:1860.3319091796875\n",
      "epoch:11, batch:900/2500, reward:1876.8441162109375\n",
      "epoch:11, batch:950/2500, reward:1868.6474609375\n",
      "epoch:11, batch:1000/2500, reward:1880.323974609375\n",
      "epoch:11, batch:1050/2500, reward:1884.375732421875\n",
      "epoch:11, batch:1100/2500, reward:1873.350341796875\n",
      "epoch:11, batch:1150/2500, reward:1891.376953125\n",
      "epoch:11, batch:1200/2500, reward:1884.90576171875\n",
      "epoch:11, batch:1250/2500, reward:2139.616455078125\n",
      "epoch:11, batch:1300/2500, reward:1910.81396484375\n",
      "epoch:11, batch:1350/2500, reward:1876.370361328125\n",
      "epoch:11, batch:1400/2500, reward:1886.112548828125\n",
      "epoch:11, batch:1450/2500, reward:1845.4307861328125\n",
      "epoch:11, batch:1500/2500, reward:1832.5841064453125\n",
      "epoch:11, batch:1550/2500, reward:1850.77587890625\n",
      "epoch:11, batch:1600/2500, reward:1845.434814453125\n",
      "epoch:11, batch:1650/2500, reward:1825.9915771484375\n",
      "epoch:11, batch:1700/2500, reward:1820.775634765625\n",
      "epoch:11, batch:1750/2500, reward:1844.00244140625\n",
      "epoch:11, batch:1800/2500, reward:1830.152099609375\n",
      "epoch:11, batch:1850/2500, reward:1837.8734130859375\n",
      "epoch:11, batch:1900/2500, reward:1818.9970703125\n",
      "epoch:11, batch:1950/2500, reward:1849.077880859375\n",
      "epoch:11, batch:2000/2500, reward:1812.854248046875\n",
      "epoch:11, batch:2050/2500, reward:2098.060546875\n",
      "epoch:11, batch:2100/2500, reward:1973.51220703125\n",
      "epoch:11, batch:2150/2500, reward:1914.88525390625\n",
      "epoch:11, batch:2200/2500, reward:1900.48876953125\n",
      "epoch:11, batch:2250/2500, reward:1857.30810546875\n",
      "epoch:11, batch:2300/2500, reward:1843.6583251953125\n",
      "epoch:11, batch:2350/2500, reward:2059.609375\n",
      "epoch:11, batch:2400/2500, reward:1849.2911376953125\n",
      "epoch:11, batch:2450/2500, reward:1816.2701416015625\n",
      "epoch:11, batch:2500/2500, reward:1817.22998046875\n",
      "Avg Actor 1792.1375732421875 --- Avg Critic 1750.18017578125\n",
      "Epoch: 11, epoch time: 75.947min, tot time: 0.630day, L_actor: 1792.138, L_critic: 1750.180, update: False\n",
      "Save Checkpoints\n",
      "epoch:12, batch:50/2500, reward:1819.1370849609375\n",
      "epoch:12, batch:100/2500, reward:1822.9114990234375\n",
      "epoch:12, batch:150/2500, reward:1820.728271484375\n",
      "epoch:12, batch:200/2500, reward:1836.6251220703125\n",
      "epoch:12, batch:250/2500, reward:1869.670166015625\n",
      "epoch:12, batch:300/2500, reward:1838.0113525390625\n",
      "epoch:12, batch:350/2500, reward:1822.07373046875\n",
      "epoch:12, batch:400/2500, reward:1814.84423828125\n",
      "epoch:12, batch:450/2500, reward:1808.61962890625\n",
      "epoch:12, batch:500/2500, reward:1847.661865234375\n",
      "epoch:12, batch:550/2500, reward:1820.4774169921875\n",
      "epoch:12, batch:600/2500, reward:1796.8114013671875\n",
      "epoch:12, batch:650/2500, reward:1795.48779296875\n",
      "epoch:12, batch:700/2500, reward:1815.3375244140625\n",
      "epoch:12, batch:750/2500, reward:1995.162841796875\n",
      "epoch:12, batch:800/2500, reward:1806.3658447265625\n",
      "epoch:12, batch:850/2500, reward:1848.772216796875\n",
      "epoch:12, batch:900/2500, reward:1797.099853515625\n",
      "epoch:12, batch:950/2500, reward:1812.4547119140625\n",
      "epoch:12, batch:1000/2500, reward:1793.3248291015625\n",
      "epoch:12, batch:1050/2500, reward:1786.912353515625\n",
      "epoch:12, batch:1100/2500, reward:1802.56982421875\n",
      "epoch:12, batch:1150/2500, reward:1792.343994140625\n",
      "epoch:12, batch:1200/2500, reward:1788.093994140625\n",
      "epoch:12, batch:1250/2500, reward:1800.4521484375\n",
      "epoch:12, batch:1300/2500, reward:1816.45166015625\n",
      "epoch:12, batch:1350/2500, reward:1814.43505859375\n",
      "epoch:12, batch:1400/2500, reward:1808.5299072265625\n",
      "epoch:12, batch:1450/2500, reward:1816.558349609375\n",
      "epoch:12, batch:1500/2500, reward:1815.9490966796875\n",
      "epoch:12, batch:1550/2500, reward:1877.8460693359375\n",
      "epoch:12, batch:1600/2500, reward:1951.1868896484375\n",
      "epoch:12, batch:1650/2500, reward:1928.881103515625\n",
      "epoch:12, batch:1700/2500, reward:1964.3291015625\n",
      "epoch:12, batch:1750/2500, reward:1879.4744873046875\n",
      "epoch:12, batch:1800/2500, reward:1831.72119140625\n",
      "epoch:12, batch:1850/2500, reward:1830.6627197265625\n",
      "epoch:12, batch:1900/2500, reward:1825.032470703125\n",
      "epoch:12, batch:1950/2500, reward:1856.956298828125\n",
      "epoch:12, batch:2000/2500, reward:1822.8389892578125\n",
      "epoch:12, batch:2050/2500, reward:1850.299072265625\n",
      "epoch:12, batch:2100/2500, reward:1857.96240234375\n",
      "epoch:12, batch:2150/2500, reward:1844.784423828125\n",
      "epoch:12, batch:2200/2500, reward:1844.4609375\n",
      "epoch:12, batch:2250/2500, reward:1894.3519287109375\n",
      "epoch:12, batch:2300/2500, reward:1848.861328125\n",
      "epoch:12, batch:2350/2500, reward:1828.4219970703125\n",
      "epoch:12, batch:2400/2500, reward:1834.34521484375\n",
      "epoch:12, batch:2450/2500, reward:1879.06689453125\n",
      "epoch:12, batch:2500/2500, reward:1855.575927734375\n",
      "Avg Actor 1761.3350830078125 --- Avg Critic 1750.18017578125\n",
      "Epoch: 12, epoch time: 75.337min, tot time: 0.683day, L_actor: 1761.335, L_critic: 1750.180, update: False\n",
      "Save Checkpoints\n",
      "epoch:13, batch:50/2500, reward:1852.827392578125\n",
      "epoch:13, batch:100/2500, reward:1829.906005859375\n",
      "epoch:13, batch:150/2500, reward:1831.83642578125\n",
      "epoch:13, batch:200/2500, reward:1805.56787109375\n",
      "epoch:13, batch:250/2500, reward:1819.247314453125\n",
      "epoch:13, batch:300/2500, reward:1873.14599609375\n",
      "epoch:13, batch:350/2500, reward:1842.991455078125\n",
      "epoch:13, batch:400/2500, reward:1817.1279296875\n",
      "epoch:13, batch:450/2500, reward:1831.40185546875\n",
      "epoch:13, batch:500/2500, reward:1866.021240234375\n",
      "epoch:13, batch:550/2500, reward:3181.21240234375\n",
      "epoch:13, batch:600/2500, reward:4054.87353515625\n",
      "epoch:13, batch:650/2500, reward:3233.140869140625\n",
      "epoch:13, batch:700/2500, reward:3008.9150390625\n",
      "epoch:13, batch:750/2500, reward:2547.869384765625\n",
      "epoch:13, batch:800/2500, reward:2367.383544921875\n",
      "epoch:13, batch:850/2500, reward:2281.250732421875\n",
      "epoch:13, batch:900/2500, reward:2191.888671875\n",
      "epoch:13, batch:950/2500, reward:2145.3740234375\n",
      "epoch:13, batch:1000/2500, reward:2155.51220703125\n",
      "epoch:13, batch:1050/2500, reward:2126.30078125\n",
      "epoch:13, batch:1100/2500, reward:2041.8187255859375\n",
      "epoch:13, batch:1150/2500, reward:2002.81201171875\n",
      "epoch:13, batch:1200/2500, reward:1987.76806640625\n",
      "epoch:13, batch:1250/2500, reward:1991.623291015625\n",
      "epoch:13, batch:1300/2500, reward:1957.8704833984375\n",
      "epoch:13, batch:1350/2500, reward:1976.080322265625\n",
      "epoch:13, batch:1400/2500, reward:1967.5711669921875\n",
      "epoch:13, batch:1450/2500, reward:1947.3402099609375\n",
      "epoch:13, batch:1500/2500, reward:1941.2645263671875\n",
      "epoch:13, batch:1550/2500, reward:1954.85888671875\n",
      "epoch:13, batch:1600/2500, reward:1925.105224609375\n",
      "epoch:13, batch:1650/2500, reward:1914.72802734375\n",
      "epoch:13, batch:1700/2500, reward:1919.6015625\n",
      "epoch:13, batch:1750/2500, reward:1914.597900390625\n",
      "epoch:13, batch:1800/2500, reward:1922.601806640625\n",
      "epoch:13, batch:1850/2500, reward:1925.1829833984375\n",
      "epoch:13, batch:1900/2500, reward:1916.08544921875\n",
      "epoch:13, batch:1950/2500, reward:1900.27685546875\n",
      "epoch:13, batch:2000/2500, reward:1908.7010498046875\n",
      "epoch:13, batch:2050/2500, reward:1943.9898681640625\n",
      "epoch:13, batch:2100/2500, reward:1910.447265625\n",
      "epoch:13, batch:2150/2500, reward:1879.1434326171875\n",
      "epoch:13, batch:2200/2500, reward:1871.113525390625\n",
      "epoch:13, batch:2250/2500, reward:1874.83544921875\n",
      "epoch:13, batch:2300/2500, reward:1861.2091064453125\n",
      "epoch:13, batch:2350/2500, reward:1884.42138671875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:13, batch:2400/2500, reward:1836.0997314453125\n",
      "epoch:13, batch:2450/2500, reward:1851.8199462890625\n",
      "epoch:13, batch:2500/2500, reward:1821.6884765625\n",
      "Avg Actor 1772.3426513671875 --- Avg Critic 1750.18017578125\n",
      "Epoch: 13, epoch time: 74.692min, tot time: 0.735day, L_actor: 1772.343, L_critic: 1750.180, update: False\n",
      "Save Checkpoints\n",
      "epoch:14, batch:50/2500, reward:1979.0013427734375\n",
      "epoch:14, batch:100/2500, reward:1849.139404296875\n",
      "epoch:14, batch:150/2500, reward:1826.95361328125\n",
      "epoch:14, batch:200/2500, reward:1822.865966796875\n",
      "epoch:14, batch:250/2500, reward:1839.1815185546875\n",
      "epoch:14, batch:300/2500, reward:1833.7564697265625\n",
      "epoch:14, batch:350/2500, reward:1840.49169921875\n",
      "epoch:14, batch:400/2500, reward:1831.485107421875\n",
      "epoch:14, batch:450/2500, reward:1837.6416015625\n",
      "epoch:14, batch:500/2500, reward:1846.573486328125\n",
      "epoch:14, batch:550/2500, reward:1842.37353515625\n",
      "epoch:14, batch:600/2500, reward:1816.302490234375\n",
      "epoch:14, batch:650/2500, reward:1803.03955078125\n",
      "epoch:14, batch:700/2500, reward:1820.331298828125\n",
      "epoch:14, batch:750/2500, reward:1902.9583740234375\n",
      "epoch:14, batch:800/2500, reward:1896.63134765625\n",
      "epoch:14, batch:850/2500, reward:1876.164306640625\n",
      "epoch:14, batch:900/2500, reward:1872.3873291015625\n",
      "epoch:14, batch:950/2500, reward:1853.703125\n",
      "epoch:14, batch:1000/2500, reward:1830.8924560546875\n",
      "epoch:14, batch:1050/2500, reward:1865.95947265625\n",
      "epoch:14, batch:1100/2500, reward:1841.06298828125\n",
      "epoch:14, batch:1150/2500, reward:1867.941162109375\n",
      "epoch:14, batch:1200/2500, reward:1858.528564453125\n",
      "epoch:14, batch:1250/2500, reward:1831.771728515625\n",
      "epoch:14, batch:1300/2500, reward:1829.76904296875\n",
      "epoch:14, batch:1350/2500, reward:1819.709716796875\n",
      "epoch:14, batch:1400/2500, reward:1801.271484375\n",
      "epoch:14, batch:1450/2500, reward:1899.04345703125\n",
      "epoch:14, batch:1500/2500, reward:1868.021240234375\n",
      "epoch:14, batch:1550/2500, reward:1830.2515869140625\n",
      "epoch:14, batch:1600/2500, reward:1809.043701171875\n",
      "epoch:14, batch:1650/2500, reward:1827.822509765625\n",
      "epoch:14, batch:1700/2500, reward:1808.9097900390625\n",
      "epoch:14, batch:1750/2500, reward:1871.8779296875\n",
      "epoch:14, batch:1800/2500, reward:1852.4053955078125\n",
      "epoch:14, batch:1850/2500, reward:1969.71826171875\n",
      "epoch:14, batch:1900/2500, reward:1947.041259765625\n",
      "epoch:14, batch:1950/2500, reward:1882.078369140625\n",
      "epoch:14, batch:2000/2500, reward:1863.9122314453125\n",
      "epoch:14, batch:2050/2500, reward:1848.27001953125\n",
      "epoch:14, batch:2100/2500, reward:1844.2437744140625\n",
      "epoch:14, batch:2150/2500, reward:1838.7255859375\n",
      "epoch:14, batch:2200/2500, reward:1810.985107421875\n",
      "epoch:14, batch:2250/2500, reward:1825.397705078125\n",
      "epoch:14, batch:2300/2500, reward:1857.218505859375\n",
      "epoch:14, batch:2350/2500, reward:1810.0673828125\n",
      "epoch:14, batch:2400/2500, reward:1823.710205078125\n",
      "epoch:14, batch:2450/2500, reward:1839.527587890625\n",
      "epoch:14, batch:2500/2500, reward:1812.4586181640625\n",
      "Avg Actor 1736.8966064453125 --- Avg Critic 1750.18017578125\n",
      "My actor is going on the right road Hallelujah :) Updated\n",
      "Epoch: 14, epoch time: 75.518min, tot time: 0.788day, L_actor: 1736.897, L_critic: 1750.180, update: True\n",
      "Save Checkpoints\n",
      "epoch:15, batch:50/2500, reward:1823.5926513671875\n",
      "epoch:15, batch:100/2500, reward:1828.0902099609375\n",
      "epoch:15, batch:150/2500, reward:1806.29150390625\n",
      "epoch:15, batch:200/2500, reward:1794.0885009765625\n",
      "epoch:15, batch:250/2500, reward:1858.823974609375\n",
      "epoch:15, batch:300/2500, reward:1790.88671875\n",
      "epoch:15, batch:350/2500, reward:1815.9873046875\n",
      "epoch:15, batch:400/2500, reward:1809.614501953125\n",
      "epoch:15, batch:450/2500, reward:1795.34716796875\n",
      "epoch:15, batch:500/2500, reward:1817.48095703125\n",
      "epoch:15, batch:550/2500, reward:1823.77001953125\n",
      "epoch:15, batch:600/2500, reward:1775.502685546875\n",
      "epoch:15, batch:650/2500, reward:1824.20068359375\n",
      "epoch:15, batch:700/2500, reward:1815.1414794921875\n",
      "epoch:15, batch:750/2500, reward:1777.5072021484375\n",
      "epoch:15, batch:800/2500, reward:1802.678955078125\n",
      "epoch:15, batch:850/2500, reward:1806.067626953125\n",
      "epoch:15, batch:900/2500, reward:1868.135498046875\n",
      "epoch:15, batch:950/2500, reward:1833.0167236328125\n",
      "epoch:15, batch:1000/2500, reward:1805.81201171875\n",
      "epoch:15, batch:1050/2500, reward:1846.14208984375\n",
      "epoch:15, batch:1100/2500, reward:1823.601318359375\n",
      "epoch:15, batch:1150/2500, reward:1855.540283203125\n",
      "epoch:15, batch:1200/2500, reward:1787.4212646484375\n",
      "epoch:15, batch:1250/2500, reward:1804.8447265625\n",
      "epoch:15, batch:1300/2500, reward:1778.1973876953125\n",
      "epoch:15, batch:1350/2500, reward:1787.34912109375\n",
      "epoch:15, batch:1400/2500, reward:1787.1353759765625\n",
      "epoch:15, batch:1450/2500, reward:1820.70556640625\n",
      "epoch:15, batch:1500/2500, reward:1827.2366943359375\n",
      "epoch:15, batch:1550/2500, reward:1804.474853515625\n",
      "epoch:15, batch:1600/2500, reward:1805.2205810546875\n",
      "epoch:15, batch:1650/2500, reward:1819.6319580078125\n",
      "epoch:15, batch:1700/2500, reward:1809.198486328125\n",
      "epoch:15, batch:1750/2500, reward:1845.28759765625\n",
      "epoch:15, batch:1800/2500, reward:1838.65380859375\n",
      "epoch:15, batch:1850/2500, reward:2006.357421875\n",
      "epoch:15, batch:1900/2500, reward:1857.179931640625\n",
      "epoch:15, batch:1950/2500, reward:1865.7777099609375\n",
      "epoch:15, batch:2000/2500, reward:1813.47265625\n",
      "epoch:15, batch:2050/2500, reward:1806.899658203125\n",
      "epoch:15, batch:2100/2500, reward:1845.9892578125\n",
      "epoch:15, batch:2150/2500, reward:1819.224609375\n",
      "epoch:15, batch:2200/2500, reward:1810.5328369140625\n",
      "epoch:15, batch:2250/2500, reward:1794.5211181640625\n",
      "epoch:15, batch:2300/2500, reward:1778.557373046875\n",
      "epoch:15, batch:2350/2500, reward:1808.516357421875\n",
      "epoch:15, batch:2400/2500, reward:1796.9375\n",
      "epoch:15, batch:2450/2500, reward:1776.4127197265625\n",
      "epoch:15, batch:2500/2500, reward:1784.068115234375\n",
      "Avg Actor 1896.098876953125 --- Avg Critic 1736.8966064453125\n",
      "Epoch: 15, epoch time: 74.822min, tot time: 0.840day, L_actor: 1896.099, L_critic: 1736.897, update: False\n",
      "Save Checkpoints\n",
      "epoch:16, batch:50/2500, reward:1808.5728759765625\n",
      "epoch:16, batch:100/2500, reward:1804.154296875\n",
      "epoch:16, batch:150/2500, reward:1763.5931396484375\n",
      "epoch:16, batch:200/2500, reward:1772.9000244140625\n",
      "epoch:16, batch:250/2500, reward:1795.9493408203125\n",
      "epoch:16, batch:300/2500, reward:1804.0556640625\n",
      "epoch:16, batch:350/2500, reward:1764.1925048828125\n",
      "epoch:16, batch:400/2500, reward:1776.678955078125\n",
      "epoch:16, batch:450/2500, reward:1768.219970703125\n",
      "epoch:16, batch:500/2500, reward:1747.6771240234375\n",
      "epoch:16, batch:550/2500, reward:1786.2440185546875\n",
      "epoch:16, batch:600/2500, reward:1751.9410400390625\n",
      "epoch:16, batch:650/2500, reward:1762.8238525390625\n",
      "epoch:16, batch:700/2500, reward:1771.9735107421875\n",
      "epoch:16, batch:750/2500, reward:1764.2191162109375\n",
      "epoch:16, batch:800/2500, reward:1762.112060546875\n",
      "epoch:16, batch:850/2500, reward:1853.134033203125\n",
      "epoch:16, batch:900/2500, reward:3038.724365234375\n",
      "epoch:16, batch:950/2500, reward:2253.70068359375\n",
      "epoch:16, batch:1000/2500, reward:2136.75537109375\n",
      "epoch:16, batch:1050/2500, reward:2125.727294921875\n",
      "epoch:16, batch:1100/2500, reward:2129.49267578125\n",
      "epoch:16, batch:1150/2500, reward:2100.89501953125\n",
      "epoch:16, batch:1200/2500, reward:2093.06396484375\n",
      "epoch:16, batch:1250/2500, reward:2064.4326171875\n",
      "epoch:16, batch:1300/2500, reward:2077.787841796875\n",
      "epoch:16, batch:1350/2500, reward:2063.138671875\n",
      "epoch:16, batch:1400/2500, reward:2040.41748046875\n",
      "epoch:16, batch:1450/2500, reward:1940.071044921875\n",
      "epoch:16, batch:1500/2500, reward:1868.49560546875\n",
      "epoch:16, batch:1550/2500, reward:1864.6259765625\n",
      "epoch:16, batch:1600/2500, reward:1851.65185546875\n",
      "epoch:16, batch:1650/2500, reward:1879.598388671875\n",
      "epoch:16, batch:1700/2500, reward:1866.64892578125\n",
      "epoch:16, batch:1750/2500, reward:1859.1468505859375\n",
      "epoch:16, batch:1800/2500, reward:1863.4176025390625\n",
      "epoch:16, batch:1850/2500, reward:1867.761474609375\n",
      "epoch:16, batch:1900/2500, reward:1858.22900390625\n",
      "epoch:16, batch:1950/2500, reward:1832.84375\n",
      "epoch:16, batch:2000/2500, reward:1846.5369873046875\n",
      "epoch:16, batch:2050/2500, reward:1886.3084716796875\n",
      "epoch:16, batch:2100/2500, reward:1864.2347412109375\n",
      "epoch:16, batch:2150/2500, reward:1851.489990234375\n",
      "epoch:16, batch:2200/2500, reward:1849.023193359375\n",
      "epoch:16, batch:2250/2500, reward:1948.0452880859375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:16, batch:2300/2500, reward:1857.5472412109375\n",
      "epoch:16, batch:2350/2500, reward:1859.2587890625\n",
      "epoch:16, batch:2400/2500, reward:1859.31103515625\n",
      "epoch:16, batch:2450/2500, reward:1849.05517578125\n",
      "epoch:16, batch:2500/2500, reward:1847.509033203125\n",
      "Avg Actor 1854.9547119140625 --- Avg Critic 1736.8966064453125\n",
      "Epoch: 16, epoch time: 74.549min, tot time: 0.892day, L_actor: 1854.955, L_critic: 1736.897, update: False\n",
      "Save Checkpoints\n",
      "epoch:17, batch:50/2500, reward:2207.6884765625\n",
      "epoch:17, batch:100/2500, reward:1965.01171875\n",
      "epoch:17, batch:150/2500, reward:1873.121337890625\n",
      "epoch:17, batch:200/2500, reward:1920.49658203125\n",
      "epoch:17, batch:250/2500, reward:1901.833740234375\n",
      "epoch:17, batch:300/2500, reward:1881.868896484375\n",
      "epoch:17, batch:350/2500, reward:1900.349853515625\n",
      "epoch:17, batch:400/2500, reward:1888.53173828125\n",
      "epoch:17, batch:450/2500, reward:1871.720458984375\n",
      "epoch:17, batch:500/2500, reward:1885.1712646484375\n",
      "epoch:17, batch:550/2500, reward:1883.246337890625\n",
      "epoch:17, batch:600/2500, reward:1884.44970703125\n",
      "epoch:17, batch:650/2500, reward:1921.4366455078125\n",
      "epoch:17, batch:700/2500, reward:1837.41943359375\n",
      "epoch:17, batch:750/2500, reward:1886.83056640625\n",
      "epoch:17, batch:800/2500, reward:1890.292724609375\n",
      "epoch:17, batch:850/2500, reward:1885.9359130859375\n",
      "epoch:17, batch:900/2500, reward:1958.419677734375\n",
      "epoch:17, batch:950/2500, reward:1838.1934814453125\n",
      "epoch:17, batch:1000/2500, reward:1849.24267578125\n",
      "epoch:17, batch:1050/2500, reward:1862.10595703125\n",
      "epoch:17, batch:1100/2500, reward:1902.23583984375\n",
      "epoch:17, batch:1150/2500, reward:1840.317138671875\n",
      "epoch:17, batch:1200/2500, reward:1856.430419921875\n",
      "epoch:17, batch:1250/2500, reward:1839.53125\n",
      "epoch:17, batch:1300/2500, reward:1864.1812744140625\n",
      "epoch:17, batch:1350/2500, reward:1837.70703125\n",
      "epoch:17, batch:1400/2500, reward:1835.087890625\n",
      "epoch:17, batch:1450/2500, reward:1812.2908935546875\n",
      "epoch:17, batch:1500/2500, reward:1842.10107421875\n",
      "epoch:17, batch:1550/2500, reward:1844.001708984375\n",
      "epoch:17, batch:1600/2500, reward:1836.426513671875\n",
      "epoch:17, batch:1650/2500, reward:1830.134765625\n",
      "epoch:17, batch:1700/2500, reward:1848.820068359375\n",
      "epoch:17, batch:1750/2500, reward:1802.585693359375\n",
      "epoch:17, batch:1800/2500, reward:1867.323486328125\n",
      "epoch:17, batch:1850/2500, reward:1908.232177734375\n",
      "epoch:17, batch:1900/2500, reward:1896.9927978515625\n",
      "epoch:17, batch:1950/2500, reward:1905.8707275390625\n",
      "epoch:17, batch:2000/2500, reward:1845.3193359375\n",
      "epoch:17, batch:2050/2500, reward:1879.0809326171875\n",
      "epoch:17, batch:2100/2500, reward:1817.1195068359375\n",
      "epoch:17, batch:2150/2500, reward:1810.5478515625\n",
      "epoch:17, batch:2200/2500, reward:1794.576904296875\n",
      "epoch:17, batch:2250/2500, reward:1813.9808349609375\n",
      "epoch:17, batch:2300/2500, reward:1793.404296875\n",
      "epoch:17, batch:2350/2500, reward:1793.957275390625\n",
      "epoch:17, batch:2400/2500, reward:1785.6351318359375\n",
      "epoch:17, batch:2450/2500, reward:1794.529296875\n",
      "epoch:17, batch:2500/2500, reward:1787.12060546875\n",
      "Avg Actor 1738.7664794921875 --- Avg Critic 1736.8966064453125\n",
      "Epoch: 17, epoch time: 74.630min, tot time: 0.945day, L_actor: 1738.766, L_critic: 1736.897, update: False\n",
      "Save Checkpoints\n",
      "epoch:18, batch:50/2500, reward:1796.4853515625\n",
      "epoch:18, batch:100/2500, reward:1769.50146484375\n",
      "epoch:18, batch:150/2500, reward:1780.0673828125\n",
      "epoch:18, batch:200/2500, reward:1786.613037109375\n",
      "epoch:18, batch:250/2500, reward:1801.1673583984375\n",
      "epoch:18, batch:300/2500, reward:1774.215576171875\n",
      "epoch:18, batch:350/2500, reward:1806.83984375\n",
      "epoch:18, batch:400/2500, reward:1794.858642578125\n",
      "epoch:18, batch:450/2500, reward:1815.2701416015625\n",
      "epoch:18, batch:500/2500, reward:1788.85400390625\n",
      "epoch:18, batch:550/2500, reward:1804.3624267578125\n",
      "epoch:18, batch:600/2500, reward:1756.055419921875\n",
      "epoch:18, batch:650/2500, reward:1773.54443359375\n",
      "epoch:18, batch:700/2500, reward:1754.63671875\n",
      "epoch:18, batch:750/2500, reward:1804.1942138671875\n",
      "epoch:18, batch:800/2500, reward:1785.23095703125\n",
      "epoch:18, batch:850/2500, reward:1809.2218017578125\n",
      "epoch:18, batch:900/2500, reward:1805.8896484375\n",
      "epoch:18, batch:950/2500, reward:1767.65673828125\n",
      "epoch:18, batch:1000/2500, reward:1760.889892578125\n",
      "epoch:18, batch:1050/2500, reward:1768.6837158203125\n",
      "epoch:18, batch:1100/2500, reward:1782.028076171875\n",
      "epoch:18, batch:1150/2500, reward:1788.54150390625\n",
      "epoch:18, batch:1200/2500, reward:1811.4912109375\n",
      "epoch:18, batch:1250/2500, reward:1818.49365234375\n",
      "epoch:18, batch:1300/2500, reward:1770.5528564453125\n",
      "epoch:18, batch:1350/2500, reward:1791.498779296875\n",
      "epoch:18, batch:1400/2500, reward:1837.65087890625\n",
      "epoch:18, batch:1450/2500, reward:2029.37353515625\n",
      "epoch:18, batch:1500/2500, reward:1888.651611328125\n",
      "epoch:18, batch:1550/2500, reward:1848.197021484375\n",
      "epoch:18, batch:1600/2500, reward:1836.2864990234375\n",
      "epoch:18, batch:1650/2500, reward:1801.184326171875\n",
      "epoch:18, batch:1700/2500, reward:1816.751708984375\n",
      "epoch:18, batch:1750/2500, reward:1770.4515380859375\n",
      "epoch:18, batch:1800/2500, reward:1779.896728515625\n",
      "epoch:18, batch:1850/2500, reward:1757.846923828125\n",
      "epoch:18, batch:1900/2500, reward:1762.5794677734375\n",
      "epoch:18, batch:1950/2500, reward:1958.107666015625\n",
      "epoch:18, batch:2000/2500, reward:1792.97314453125\n",
      "epoch:18, batch:2050/2500, reward:1775.428955078125\n",
      "epoch:18, batch:2100/2500, reward:1811.560302734375\n",
      "epoch:18, batch:2150/2500, reward:1789.25927734375\n",
      "epoch:18, batch:2200/2500, reward:1790.6192626953125\n",
      "epoch:18, batch:2250/2500, reward:1792.1524658203125\n",
      "epoch:18, batch:2300/2500, reward:1764.73388671875\n",
      "epoch:18, batch:2350/2500, reward:1773.7021484375\n",
      "epoch:18, batch:2400/2500, reward:1767.88134765625\n",
      "epoch:18, batch:2450/2500, reward:1766.6385498046875\n",
      "epoch:18, batch:2500/2500, reward:1747.111328125\n",
      "Avg Actor 1763.8076171875 --- Avg Critic 1736.8966064453125\n",
      "Epoch: 18, epoch time: 74.898min, tot time: 0.997day, L_actor: 1763.808, L_critic: 1736.897, update: False\n",
      "Save Checkpoints\n",
      "epoch:19, batch:50/2500, reward:1884.712890625\n",
      "epoch:19, batch:100/2500, reward:1777.10986328125\n",
      "epoch:19, batch:150/2500, reward:1761.44775390625\n",
      "epoch:19, batch:200/2500, reward:1768.69873046875\n",
      "epoch:19, batch:250/2500, reward:1766.96337890625\n",
      "epoch:19, batch:300/2500, reward:1739.7022705078125\n",
      "epoch:19, batch:350/2500, reward:1754.9869384765625\n",
      "epoch:19, batch:400/2500, reward:1747.83642578125\n",
      "epoch:19, batch:450/2500, reward:1760.6033935546875\n",
      "epoch:19, batch:500/2500, reward:1754.702880859375\n",
      "epoch:19, batch:550/2500, reward:1754.3948974609375\n",
      "epoch:19, batch:600/2500, reward:1737.9549560546875\n",
      "epoch:19, batch:650/2500, reward:1754.9541015625\n",
      "epoch:19, batch:700/2500, reward:1750.1243896484375\n",
      "epoch:19, batch:750/2500, reward:1750.1591796875\n",
      "epoch:19, batch:800/2500, reward:1749.3463134765625\n",
      "epoch:19, batch:850/2500, reward:1772.591552734375\n",
      "epoch:19, batch:900/2500, reward:1758.921630859375\n",
      "epoch:19, batch:950/2500, reward:1751.051025390625\n",
      "epoch:19, batch:1000/2500, reward:1751.7410888671875\n",
      "epoch:19, batch:1050/2500, reward:1760.2342529296875\n",
      "epoch:19, batch:1100/2500, reward:1744.403076171875\n",
      "epoch:19, batch:1150/2500, reward:1752.2481689453125\n",
      "epoch:19, batch:1200/2500, reward:1785.65478515625\n",
      "epoch:19, batch:1250/2500, reward:1759.867431640625\n",
      "epoch:19, batch:1300/2500, reward:1756.30810546875\n",
      "epoch:19, batch:1350/2500, reward:1807.1083984375\n",
      "epoch:19, batch:1400/2500, reward:1799.745849609375\n",
      "epoch:19, batch:1450/2500, reward:1787.35009765625\n",
      "epoch:19, batch:1500/2500, reward:1824.813720703125\n",
      "epoch:19, batch:1550/2500, reward:1765.7796630859375\n",
      "epoch:19, batch:1600/2500, reward:1777.978515625\n",
      "epoch:19, batch:1650/2500, reward:1760.04638671875\n",
      "epoch:19, batch:1700/2500, reward:1723.65966796875\n",
      "epoch:19, batch:1750/2500, reward:1730.47607421875\n",
      "epoch:19, batch:1800/2500, reward:1738.781494140625\n",
      "epoch:19, batch:1850/2500, reward:1726.165771484375\n",
      "epoch:19, batch:1900/2500, reward:1718.6669921875\n",
      "epoch:19, batch:1950/2500, reward:1732.3021240234375\n",
      "epoch:19, batch:2000/2500, reward:1723.4598388671875\n",
      "epoch:19, batch:2050/2500, reward:1739.2314453125\n",
      "epoch:19, batch:2100/2500, reward:1731.973876953125\n",
      "epoch:19, batch:2150/2500, reward:1728.792724609375\n",
      "epoch:19, batch:2200/2500, reward:1741.21826171875\n",
      "epoch:19, batch:2250/2500, reward:1727.45849609375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:19, batch:2300/2500, reward:1742.6630859375\n",
      "epoch:19, batch:2350/2500, reward:1727.97119140625\n",
      "epoch:19, batch:2400/2500, reward:1723.521240234375\n",
      "epoch:19, batch:2450/2500, reward:1717.5660400390625\n",
      "epoch:19, batch:2500/2500, reward:1775.288818359375\n",
      "Avg Actor 1998.0230712890625 --- Avg Critic 1736.8966064453125\n",
      "Epoch: 19, epoch time: 74.118min, tot time: 1.049day, L_actor: 1998.023, L_critic: 1736.897, update: False\n",
      "Save Checkpoints\n",
      "epoch:20, batch:50/2500, reward:1877.6875\n",
      "epoch:20, batch:100/2500, reward:1721.2078857421875\n",
      "epoch:20, batch:150/2500, reward:1732.0404052734375\n",
      "epoch:20, batch:200/2500, reward:1726.259521484375\n",
      "epoch:20, batch:250/2500, reward:1736.3077392578125\n",
      "epoch:20, batch:300/2500, reward:1726.693359375\n",
      "epoch:20, batch:350/2500, reward:1731.664794921875\n",
      "epoch:20, batch:400/2500, reward:1737.1107177734375\n",
      "epoch:20, batch:450/2500, reward:1724.71728515625\n",
      "epoch:20, batch:500/2500, reward:1719.7984619140625\n",
      "epoch:20, batch:550/2500, reward:1754.15380859375\n",
      "epoch:20, batch:600/2500, reward:1756.625732421875\n",
      "epoch:20, batch:650/2500, reward:1949.6893310546875\n",
      "epoch:20, batch:700/2500, reward:1815.558349609375\n",
      "epoch:20, batch:750/2500, reward:1817.689453125\n",
      "epoch:20, batch:800/2500, reward:1794.17626953125\n",
      "epoch:20, batch:850/2500, reward:1774.2620849609375\n",
      "epoch:20, batch:900/2500, reward:1784.684326171875\n",
      "epoch:20, batch:950/2500, reward:1781.912109375\n",
      "epoch:20, batch:1000/2500, reward:1773.9967041015625\n",
      "epoch:20, batch:1050/2500, reward:1788.706787109375\n",
      "epoch:20, batch:1100/2500, reward:1770.89404296875\n",
      "epoch:20, batch:1150/2500, reward:1777.01123046875\n",
      "epoch:20, batch:1200/2500, reward:1767.165771484375\n",
      "epoch:20, batch:1250/2500, reward:1741.282958984375\n",
      "epoch:20, batch:1300/2500, reward:1803.0679931640625\n",
      "epoch:20, batch:1350/2500, reward:1742.51171875\n",
      "epoch:20, batch:1400/2500, reward:1739.94580078125\n",
      "epoch:20, batch:1450/2500, reward:1733.786376953125\n",
      "epoch:20, batch:1500/2500, reward:1724.432373046875\n",
      "epoch:20, batch:1550/2500, reward:1720.98583984375\n",
      "epoch:20, batch:1600/2500, reward:1733.15625\n",
      "epoch:20, batch:1650/2500, reward:1726.411865234375\n",
      "epoch:20, batch:1700/2500, reward:1725.9134521484375\n",
      "epoch:20, batch:1750/2500, reward:2075.587890625\n",
      "epoch:20, batch:1800/2500, reward:2207.99267578125\n",
      "epoch:20, batch:1850/2500, reward:1969.709716796875\n",
      "epoch:20, batch:1900/2500, reward:1889.3087158203125\n",
      "epoch:20, batch:1950/2500, reward:1876.65771484375\n",
      "epoch:20, batch:2000/2500, reward:1868.0701904296875\n",
      "epoch:20, batch:2050/2500, reward:1837.080078125\n",
      "epoch:20, batch:2100/2500, reward:1847.7349853515625\n",
      "epoch:20, batch:2150/2500, reward:1853.70361328125\n",
      "epoch:20, batch:2200/2500, reward:1860.177490234375\n",
      "epoch:20, batch:2250/2500, reward:1833.5423583984375\n",
      "epoch:20, batch:2300/2500, reward:1834.4910888671875\n",
      "epoch:20, batch:2350/2500, reward:1973.599853515625\n",
      "epoch:20, batch:2400/2500, reward:1884.8499755859375\n",
      "epoch:20, batch:2450/2500, reward:1831.5179443359375\n",
      "epoch:20, batch:2500/2500, reward:1855.3028564453125\n",
      "Avg Actor 1770.74609375 --- Avg Critic 1736.8966064453125\n",
      "Epoch: 20, epoch time: 74.071min, tot time: 1.101day, L_actor: 1770.746, L_critic: 1736.897, update: False\n",
      "Save Checkpoints\n",
      "epoch:21, batch:50/2500, reward:1813.8819580078125\n",
      "epoch:21, batch:100/2500, reward:1811.5697021484375\n",
      "epoch:21, batch:150/2500, reward:1824.9423828125\n",
      "epoch:21, batch:200/2500, reward:1819.0263671875\n",
      "epoch:21, batch:250/2500, reward:1807.43798828125\n",
      "epoch:21, batch:300/2500, reward:1819.64013671875\n",
      "epoch:21, batch:350/2500, reward:1839.06787109375\n",
      "epoch:21, batch:400/2500, reward:1869.44482421875\n",
      "epoch:21, batch:450/2500, reward:1849.2681884765625\n",
      "epoch:21, batch:500/2500, reward:1806.8372802734375\n",
      "epoch:21, batch:550/2500, reward:1823.182373046875\n",
      "epoch:21, batch:600/2500, reward:1816.6741943359375\n",
      "epoch:21, batch:650/2500, reward:1836.481201171875\n",
      "epoch:21, batch:700/2500, reward:1864.344970703125\n",
      "epoch:21, batch:750/2500, reward:1817.91552734375\n",
      "epoch:21, batch:800/2500, reward:1847.062744140625\n",
      "epoch:21, batch:850/2500, reward:1818.1654052734375\n",
      "epoch:21, batch:900/2500, reward:1838.594970703125\n",
      "epoch:21, batch:950/2500, reward:1828.1510009765625\n",
      "epoch:21, batch:1000/2500, reward:1806.377685546875\n",
      "epoch:21, batch:1050/2500, reward:1857.800537109375\n",
      "epoch:21, batch:1100/2500, reward:1903.768310546875\n",
      "epoch:21, batch:1150/2500, reward:1850.053955078125\n",
      "epoch:21, batch:1200/2500, reward:1807.831298828125\n",
      "epoch:21, batch:1250/2500, reward:1835.1671142578125\n",
      "epoch:21, batch:1300/2500, reward:1853.2725830078125\n",
      "epoch:21, batch:1350/2500, reward:1800.67578125\n",
      "epoch:21, batch:1400/2500, reward:1809.074951171875\n",
      "epoch:21, batch:1450/2500, reward:1829.4427490234375\n",
      "epoch:21, batch:1500/2500, reward:1797.416748046875\n",
      "epoch:21, batch:1550/2500, reward:1828.648193359375\n",
      "epoch:21, batch:1600/2500, reward:1852.4742431640625\n",
      "epoch:21, batch:1650/2500, reward:1846.302978515625\n",
      "epoch:21, batch:1700/2500, reward:1863.366943359375\n",
      "epoch:21, batch:1750/2500, reward:1828.0845947265625\n",
      "epoch:21, batch:1800/2500, reward:1809.516357421875\n",
      "epoch:21, batch:1850/2500, reward:1810.4920654296875\n",
      "epoch:21, batch:1900/2500, reward:1829.486572265625\n",
      "epoch:21, batch:1950/2500, reward:1794.0615234375\n",
      "epoch:21, batch:2000/2500, reward:1847.50048828125\n",
      "epoch:21, batch:2050/2500, reward:1784.5635986328125\n",
      "epoch:21, batch:2100/2500, reward:1805.8804931640625\n",
      "epoch:21, batch:2150/2500, reward:1789.6781005859375\n",
      "epoch:21, batch:2200/2500, reward:1786.669677734375\n",
      "epoch:21, batch:2250/2500, reward:1751.533203125\n",
      "epoch:21, batch:2300/2500, reward:1771.6326904296875\n",
      "epoch:21, batch:2350/2500, reward:1762.4329833984375\n",
      "epoch:21, batch:2400/2500, reward:1752.4390869140625\n",
      "epoch:21, batch:2450/2500, reward:1747.84130859375\n",
      "epoch:21, batch:2500/2500, reward:1745.29345703125\n",
      "Avg Actor 1731.38916015625 --- Avg Critic 1736.8966064453125\n",
      "My actor is going on the right road Hallelujah :) Updated\n",
      "Epoch: 21, epoch time: 73.806min, tot time: 1.152day, L_actor: 1731.389, L_critic: 1736.897, update: True\n",
      "Save Checkpoints\n",
      "epoch:22, batch:50/2500, reward:1742.0611572265625\n",
      "epoch:22, batch:100/2500, reward:1740.4327392578125\n",
      "epoch:22, batch:150/2500, reward:1751.5263671875\n",
      "epoch:22, batch:200/2500, reward:1781.506103515625\n",
      "epoch:22, batch:250/2500, reward:1792.986083984375\n",
      "epoch:22, batch:300/2500, reward:1738.669189453125\n",
      "epoch:22, batch:350/2500, reward:1742.9261474609375\n",
      "epoch:22, batch:400/2500, reward:1869.80908203125\n",
      "epoch:22, batch:450/2500, reward:1882.60693359375\n",
      "epoch:22, batch:500/2500, reward:2111.857421875\n",
      "epoch:22, batch:550/2500, reward:2389.3564453125\n",
      "epoch:22, batch:600/2500, reward:1895.6304931640625\n",
      "epoch:22, batch:650/2500, reward:1846.3714599609375\n",
      "epoch:22, batch:700/2500, reward:2273.4345703125\n",
      "epoch:22, batch:750/2500, reward:1817.2703857421875\n",
      "epoch:22, batch:800/2500, reward:1792.398681640625\n",
      "epoch:22, batch:850/2500, reward:1794.5654296875\n",
      "epoch:22, batch:900/2500, reward:1790.093994140625\n",
      "epoch:22, batch:950/2500, reward:1785.7354736328125\n",
      "epoch:22, batch:1000/2500, reward:1786.6044921875\n",
      "epoch:22, batch:1050/2500, reward:1796.334228515625\n",
      "epoch:22, batch:1100/2500, reward:1853.646240234375\n",
      "epoch:22, batch:1150/2500, reward:1809.928955078125\n",
      "epoch:22, batch:1200/2500, reward:1822.8341064453125\n",
      "epoch:22, batch:1250/2500, reward:1818.841552734375\n",
      "epoch:22, batch:1300/2500, reward:1855.156005859375\n",
      "epoch:22, batch:1350/2500, reward:1822.4190673828125\n",
      "epoch:22, batch:1400/2500, reward:1803.827880859375\n",
      "epoch:22, batch:1450/2500, reward:1799.775634765625\n",
      "epoch:22, batch:1500/2500, reward:1811.3807373046875\n",
      "epoch:22, batch:1550/2500, reward:1812.6807861328125\n",
      "epoch:22, batch:1600/2500, reward:1805.1435546875\n",
      "epoch:22, batch:1650/2500, reward:1833.889404296875\n",
      "epoch:22, batch:1700/2500, reward:1799.28466796875\n",
      "epoch:22, batch:1750/2500, reward:1791.1845703125\n",
      "epoch:22, batch:1800/2500, reward:1828.0908203125\n",
      "epoch:22, batch:1850/2500, reward:1807.215087890625\n",
      "epoch:22, batch:1900/2500, reward:1828.603759765625\n",
      "epoch:22, batch:1950/2500, reward:1841.05224609375\n",
      "epoch:22, batch:2000/2500, reward:1791.4580078125\n",
      "epoch:22, batch:2050/2500, reward:1791.7646484375\n",
      "epoch:22, batch:2100/2500, reward:1791.713134765625\n",
      "epoch:22, batch:2150/2500, reward:1790.446533203125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:22, batch:2200/2500, reward:1794.821533203125\n",
      "epoch:22, batch:2250/2500, reward:1791.87744140625\n",
      "epoch:22, batch:2300/2500, reward:1791.477783203125\n",
      "epoch:22, batch:2350/2500, reward:1803.329345703125\n",
      "epoch:22, batch:2400/2500, reward:1802.2476806640625\n",
      "epoch:22, batch:2450/2500, reward:1809.0767822265625\n",
      "epoch:22, batch:2500/2500, reward:1785.6749267578125\n",
      "Avg Actor 1829.537353515625 --- Avg Critic 1731.38916015625\n",
      "Epoch: 22, epoch time: 72.419min, tot time: 1.203day, L_actor: 1829.537, L_critic: 1731.389, update: False\n",
      "Save Checkpoints\n",
      "epoch:23, batch:50/2500, reward:1789.5087890625\n",
      "epoch:23, batch:100/2500, reward:1806.1717529296875\n",
      "epoch:23, batch:150/2500, reward:1804.27880859375\n",
      "epoch:23, batch:200/2500, reward:1789.326171875\n",
      "epoch:23, batch:250/2500, reward:1807.28466796875\n",
      "epoch:23, batch:300/2500, reward:1792.04443359375\n",
      "epoch:23, batch:350/2500, reward:1792.2099609375\n",
      "epoch:23, batch:400/2500, reward:1769.5572509765625\n",
      "epoch:23, batch:450/2500, reward:1791.110107421875\n",
      "epoch:23, batch:500/2500, reward:1810.0521240234375\n",
      "epoch:23, batch:550/2500, reward:1810.945556640625\n",
      "epoch:23, batch:600/2500, reward:1830.69873046875\n",
      "epoch:23, batch:650/2500, reward:1794.1593017578125\n",
      "epoch:23, batch:700/2500, reward:1794.459716796875\n",
      "epoch:23, batch:750/2500, reward:1840.5482177734375\n",
      "epoch:23, batch:800/2500, reward:1820.673095703125\n",
      "epoch:23, batch:850/2500, reward:1822.726318359375\n",
      "epoch:23, batch:900/2500, reward:1819.2669677734375\n",
      "epoch:23, batch:950/2500, reward:1780.290771484375\n",
      "epoch:23, batch:1000/2500, reward:1882.494873046875\n",
      "epoch:23, batch:1050/2500, reward:1927.3486328125\n",
      "epoch:23, batch:1100/2500, reward:1801.12451171875\n",
      "epoch:23, batch:1150/2500, reward:1792.1307373046875\n",
      "epoch:23, batch:1200/2500, reward:1823.372314453125\n",
      "epoch:23, batch:1250/2500, reward:1784.77294921875\n",
      "epoch:23, batch:1300/2500, reward:1796.025146484375\n",
      "epoch:23, batch:1350/2500, reward:1784.1685791015625\n",
      "epoch:23, batch:1400/2500, reward:1777.59912109375\n",
      "epoch:23, batch:1450/2500, reward:1819.46044921875\n",
      "epoch:23, batch:1500/2500, reward:1814.2882080078125\n",
      "epoch:23, batch:1550/2500, reward:1826.7950439453125\n",
      "epoch:23, batch:1600/2500, reward:1777.2608642578125\n",
      "epoch:23, batch:1650/2500, reward:1856.408203125\n",
      "epoch:23, batch:1700/2500, reward:1816.21875\n",
      "epoch:23, batch:1750/2500, reward:1818.9049072265625\n",
      "epoch:23, batch:1800/2500, reward:1803.8525390625\n",
      "epoch:23, batch:1850/2500, reward:1803.108154296875\n",
      "epoch:23, batch:1900/2500, reward:1796.672607421875\n",
      "epoch:23, batch:1950/2500, reward:1804.6405029296875\n",
      "epoch:23, batch:2000/2500, reward:1781.519775390625\n",
      "epoch:23, batch:2050/2500, reward:1785.522216796875\n",
      "epoch:23, batch:2100/2500, reward:1796.195068359375\n",
      "epoch:23, batch:2150/2500, reward:2265.2939453125\n",
      "epoch:23, batch:2200/2500, reward:2261.3525390625\n",
      "epoch:23, batch:2250/2500, reward:1982.06689453125\n",
      "epoch:23, batch:2300/2500, reward:1856.790283203125\n",
      "epoch:23, batch:2350/2500, reward:1874.4735107421875\n",
      "epoch:23, batch:2400/2500, reward:1815.461669921875\n",
      "epoch:23, batch:2450/2500, reward:1808.891357421875\n",
      "epoch:23, batch:2500/2500, reward:1820.7073974609375\n",
      "Avg Actor 1774.5576171875 --- Avg Critic 1731.38916015625\n",
      "Epoch: 23, epoch time: 72.202min, tot time: 1.254day, L_actor: 1774.558, L_critic: 1731.389, update: False\n",
      "Save Checkpoints\n",
      "epoch:24, batch:50/2500, reward:1811.756591796875\n",
      "epoch:24, batch:100/2500, reward:1827.981201171875\n",
      "epoch:24, batch:150/2500, reward:1813.35498046875\n",
      "epoch:24, batch:200/2500, reward:1813.24755859375\n",
      "epoch:24, batch:250/2500, reward:1814.1719970703125\n",
      "epoch:24, batch:300/2500, reward:1798.8675537109375\n",
      "epoch:24, batch:350/2500, reward:1787.4521484375\n",
      "epoch:24, batch:400/2500, reward:1780.53564453125\n",
      "epoch:24, batch:450/2500, reward:1800.17138671875\n",
      "epoch:24, batch:500/2500, reward:1793.070068359375\n",
      "epoch:24, batch:550/2500, reward:1799.8851318359375\n",
      "epoch:24, batch:600/2500, reward:1781.16015625\n",
      "epoch:24, batch:650/2500, reward:1778.5673828125\n",
      "epoch:24, batch:700/2500, reward:1770.347412109375\n",
      "epoch:24, batch:750/2500, reward:1800.015869140625\n",
      "epoch:24, batch:800/2500, reward:1774.000732421875\n",
      "epoch:24, batch:850/2500, reward:1780.839599609375\n",
      "epoch:24, batch:900/2500, reward:1781.577392578125\n",
      "epoch:24, batch:950/2500, reward:1775.580810546875\n",
      "epoch:24, batch:1000/2500, reward:1777.26318359375\n",
      "epoch:24, batch:1050/2500, reward:1779.1083984375\n",
      "epoch:24, batch:1100/2500, reward:1778.8629150390625\n",
      "epoch:24, batch:1150/2500, reward:1788.006103515625\n",
      "epoch:24, batch:1200/2500, reward:1788.1876220703125\n",
      "epoch:24, batch:1250/2500, reward:1788.947509765625\n",
      "epoch:24, batch:1300/2500, reward:1786.36767578125\n",
      "epoch:24, batch:1350/2500, reward:1794.375\n",
      "epoch:24, batch:1400/2500, reward:1763.83544921875\n",
      "epoch:24, batch:1450/2500, reward:1785.95654296875\n",
      "epoch:24, batch:1500/2500, reward:1772.958251953125\n",
      "epoch:24, batch:1550/2500, reward:1779.90283203125\n",
      "epoch:24, batch:1600/2500, reward:1782.42333984375\n",
      "epoch:24, batch:1650/2500, reward:1793.29443359375\n",
      "epoch:24, batch:1700/2500, reward:1788.890869140625\n",
      "epoch:24, batch:1750/2500, reward:1777.860595703125\n",
      "epoch:24, batch:1800/2500, reward:1778.017333984375\n",
      "epoch:24, batch:1850/2500, reward:1777.78466796875\n",
      "epoch:24, batch:1900/2500, reward:1769.1080322265625\n",
      "epoch:24, batch:1950/2500, reward:1773.076416015625\n",
      "epoch:24, batch:2000/2500, reward:2300.291015625\n",
      "epoch:24, batch:2050/2500, reward:1962.5023193359375\n",
      "epoch:24, batch:2100/2500, reward:1863.053955078125\n",
      "epoch:24, batch:2150/2500, reward:1831.0009765625\n",
      "epoch:24, batch:2200/2500, reward:1832.2012939453125\n",
      "epoch:24, batch:2250/2500, reward:1803.6273193359375\n",
      "epoch:24, batch:2300/2500, reward:1824.8953857421875\n",
      "epoch:24, batch:2350/2500, reward:1817.2955322265625\n",
      "epoch:24, batch:2400/2500, reward:1794.400146484375\n",
      "epoch:24, batch:2450/2500, reward:1812.8785400390625\n",
      "epoch:24, batch:2500/2500, reward:1806.7777099609375\n",
      "Avg Actor 1760.806884765625 --- Avg Critic 1731.38916015625\n",
      "Epoch: 24, epoch time: 72.514min, tot time: 1.304day, L_actor: 1760.807, L_critic: 1731.389, update: False\n",
      "Save Checkpoints\n",
      "epoch:25, batch:50/2500, reward:1830.921875\n",
      "epoch:25, batch:100/2500, reward:1818.0592041015625\n",
      "epoch:25, batch:150/2500, reward:1799.370361328125\n",
      "epoch:25, batch:200/2500, reward:1813.371337890625\n",
      "epoch:25, batch:250/2500, reward:1813.6259765625\n",
      "epoch:25, batch:300/2500, reward:1804.704345703125\n",
      "epoch:25, batch:350/2500, reward:1852.984130859375\n",
      "epoch:25, batch:400/2500, reward:1820.1591796875\n",
      "epoch:25, batch:450/2500, reward:1804.574951171875\n",
      "epoch:25, batch:500/2500, reward:1808.119873046875\n",
      "epoch:25, batch:550/2500, reward:1953.47607421875\n",
      "epoch:25, batch:600/2500, reward:1859.16796875\n",
      "epoch:25, batch:650/2500, reward:1828.3016357421875\n",
      "epoch:25, batch:700/2500, reward:1845.07373046875\n",
      "epoch:25, batch:750/2500, reward:1915.4483642578125\n",
      "epoch:25, batch:800/2500, reward:1800.78564453125\n",
      "epoch:25, batch:850/2500, reward:1812.4835205078125\n",
      "epoch:25, batch:900/2500, reward:1799.334228515625\n",
      "epoch:25, batch:950/2500, reward:1818.769287109375\n",
      "epoch:25, batch:1000/2500, reward:1781.5194091796875\n",
      "epoch:25, batch:1050/2500, reward:1800.2777099609375\n",
      "epoch:25, batch:1100/2500, reward:1794.3021240234375\n",
      "epoch:25, batch:1150/2500, reward:1794.1575927734375\n",
      "epoch:25, batch:1200/2500, reward:1783.021728515625\n",
      "epoch:25, batch:1250/2500, reward:1797.8399658203125\n",
      "epoch:25, batch:1300/2500, reward:1785.3150634765625\n",
      "epoch:25, batch:1350/2500, reward:1796.3865966796875\n",
      "epoch:25, batch:1400/2500, reward:1776.5079345703125\n",
      "epoch:25, batch:1450/2500, reward:1814.257080078125\n",
      "epoch:25, batch:1500/2500, reward:1786.95068359375\n",
      "epoch:25, batch:1550/2500, reward:1775.045166015625\n",
      "epoch:25, batch:1600/2500, reward:1789.5194091796875\n",
      "epoch:25, batch:1650/2500, reward:1781.8970947265625\n",
      "epoch:25, batch:1700/2500, reward:1779.758056640625\n",
      "epoch:25, batch:1750/2500, reward:1767.689697265625\n",
      "epoch:25, batch:1800/2500, reward:1779.8255615234375\n",
      "epoch:25, batch:1850/2500, reward:1789.390380859375\n",
      "epoch:25, batch:1900/2500, reward:1784.580322265625\n",
      "epoch:25, batch:1950/2500, reward:1776.5228271484375\n",
      "epoch:25, batch:2000/2500, reward:1791.2158203125\n",
      "epoch:25, batch:2050/2500, reward:1783.150146484375\n",
      "epoch:25, batch:2100/2500, reward:1823.694580078125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:25, batch:2150/2500, reward:1801.9991455078125\n",
      "epoch:25, batch:2200/2500, reward:1798.43798828125\n",
      "epoch:25, batch:2250/2500, reward:1776.4996337890625\n",
      "epoch:25, batch:2300/2500, reward:1816.720947265625\n",
      "epoch:25, batch:2350/2500, reward:1780.9912109375\n",
      "epoch:25, batch:2400/2500, reward:1786.437744140625\n",
      "epoch:25, batch:2450/2500, reward:1765.6204833984375\n",
      "epoch:25, batch:2500/2500, reward:1797.334228515625\n",
      "Avg Actor 1768.409912109375 --- Avg Critic 1731.38916015625\n",
      "Epoch: 25, epoch time: 72.355min, tot time: 1.355day, L_actor: 1768.410, L_critic: 1731.389, update: False\n",
      "Save Checkpoints\n",
      "epoch:26, batch:50/2500, reward:1787.563720703125\n",
      "epoch:26, batch:100/2500, reward:1777.07666015625\n",
      "epoch:26, batch:150/2500, reward:1790.1304931640625\n",
      "epoch:26, batch:200/2500, reward:1782.7451171875\n",
      "epoch:26, batch:250/2500, reward:1778.87841796875\n",
      "epoch:26, batch:300/2500, reward:1777.5704345703125\n",
      "epoch:26, batch:350/2500, reward:1766.549072265625\n",
      "epoch:26, batch:400/2500, reward:1771.3482666015625\n",
      "epoch:26, batch:450/2500, reward:1764.88037109375\n",
      "epoch:26, batch:500/2500, reward:1753.6837158203125\n",
      "epoch:26, batch:550/2500, reward:1767.4127197265625\n",
      "epoch:26, batch:600/2500, reward:1772.169921875\n",
      "epoch:26, batch:650/2500, reward:1770.9912109375\n",
      "epoch:26, batch:700/2500, reward:1767.455322265625\n",
      "epoch:26, batch:750/2500, reward:1778.682373046875\n",
      "epoch:26, batch:800/2500, reward:1779.343994140625\n",
      "epoch:26, batch:850/2500, reward:1758.3648681640625\n",
      "epoch:26, batch:900/2500, reward:1763.415771484375\n",
      "epoch:26, batch:950/2500, reward:1772.70556640625\n",
      "epoch:26, batch:1000/2500, reward:1787.443359375\n",
      "epoch:26, batch:1050/2500, reward:1771.85498046875\n",
      "epoch:26, batch:1100/2500, reward:1788.17236328125\n",
      "epoch:26, batch:1150/2500, reward:1787.10400390625\n",
      "epoch:26, batch:1200/2500, reward:1774.6031494140625\n",
      "epoch:26, batch:1250/2500, reward:1776.9456787109375\n",
      "epoch:26, batch:1300/2500, reward:1809.32666015625\n",
      "epoch:26, batch:1350/2500, reward:1792.46435546875\n",
      "epoch:26, batch:1400/2500, reward:1799.087890625\n",
      "epoch:26, batch:1450/2500, reward:1766.292724609375\n",
      "epoch:26, batch:1500/2500, reward:1771.8095703125\n",
      "epoch:26, batch:1550/2500, reward:1760.604736328125\n",
      "epoch:26, batch:1600/2500, reward:1775.478759765625\n",
      "epoch:26, batch:1650/2500, reward:1764.675048828125\n",
      "epoch:26, batch:1700/2500, reward:1800.68115234375\n",
      "epoch:26, batch:1750/2500, reward:1769.6287841796875\n",
      "epoch:26, batch:1800/2500, reward:1801.0775146484375\n",
      "epoch:26, batch:1850/2500, reward:1762.46923828125\n",
      "epoch:26, batch:1900/2500, reward:1757.064697265625\n",
      "epoch:26, batch:1950/2500, reward:1765.4981689453125\n",
      "epoch:26, batch:2000/2500, reward:1799.14208984375\n",
      "epoch:26, batch:2050/2500, reward:1783.6624755859375\n",
      "epoch:26, batch:2100/2500, reward:1777.4254150390625\n",
      "epoch:26, batch:2150/2500, reward:1797.0684814453125\n",
      "epoch:26, batch:2200/2500, reward:1770.3687744140625\n",
      "epoch:26, batch:2250/2500, reward:1795.69677734375\n",
      "epoch:26, batch:2300/2500, reward:1785.925048828125\n",
      "epoch:26, batch:2350/2500, reward:1763.31787109375\n",
      "epoch:26, batch:2400/2500, reward:1810.517822265625\n",
      "epoch:26, batch:2450/2500, reward:1803.7745361328125\n",
      "epoch:26, batch:2500/2500, reward:1784.8585205078125\n",
      "Avg Actor 1768.409912109375 --- Avg Critic 1731.38916015625\n",
      "Epoch: 26, epoch time: 72.296min, tot time: 1.406day, L_actor: 1768.410, L_critic: 1731.389, update: False\n",
      "Save Checkpoints\n",
      "epoch:27, batch:50/2500, reward:1768.5665283203125\n",
      "epoch:27, batch:100/2500, reward:1762.827392578125\n",
      "epoch:27, batch:150/2500, reward:1778.914794921875\n",
      "epoch:27, batch:200/2500, reward:1796.54736328125\n",
      "epoch:27, batch:250/2500, reward:1792.775390625\n",
      "epoch:27, batch:300/2500, reward:1789.9866943359375\n",
      "epoch:27, batch:350/2500, reward:1792.92333984375\n",
      "epoch:27, batch:400/2500, reward:1890.8221435546875\n",
      "epoch:27, batch:450/2500, reward:1868.2327880859375\n",
      "epoch:27, batch:500/2500, reward:1809.774169921875\n",
      "epoch:27, batch:550/2500, reward:1782.2813720703125\n",
      "epoch:27, batch:600/2500, reward:1776.130126953125\n",
      "epoch:27, batch:650/2500, reward:1853.211669921875\n",
      "epoch:27, batch:700/2500, reward:1800.7225341796875\n",
      "epoch:27, batch:750/2500, reward:1777.2528076171875\n",
      "epoch:27, batch:800/2500, reward:1767.9306640625\n",
      "epoch:27, batch:850/2500, reward:1769.32177734375\n",
      "epoch:27, batch:900/2500, reward:1784.981201171875\n",
      "epoch:27, batch:950/2500, reward:1767.845458984375\n",
      "epoch:27, batch:1000/2500, reward:1762.902587890625\n",
      "epoch:27, batch:1050/2500, reward:1779.492919921875\n",
      "epoch:27, batch:1100/2500, reward:1768.842529296875\n",
      "epoch:27, batch:1150/2500, reward:1781.3026123046875\n",
      "epoch:27, batch:1200/2500, reward:1760.527587890625\n",
      "epoch:27, batch:1250/2500, reward:1765.9683837890625\n",
      "epoch:27, batch:1300/2500, reward:1770.54248046875\n",
      "epoch:27, batch:1350/2500, reward:1766.2037353515625\n",
      "epoch:27, batch:1400/2500, reward:1777.8612060546875\n",
      "epoch:27, batch:1450/2500, reward:1768.3970947265625\n",
      "epoch:27, batch:1500/2500, reward:1778.4619140625\n",
      "epoch:27, batch:1550/2500, reward:1789.779296875\n",
      "epoch:27, batch:1600/2500, reward:1774.029052734375\n",
      "epoch:27, batch:1650/2500, reward:1756.3009033203125\n",
      "epoch:27, batch:1700/2500, reward:1763.873046875\n",
      "epoch:27, batch:1750/2500, reward:1783.252197265625\n",
      "epoch:27, batch:1800/2500, reward:1778.4771728515625\n",
      "epoch:27, batch:1850/2500, reward:1773.04052734375\n",
      "epoch:27, batch:1900/2500, reward:1759.694091796875\n",
      "epoch:27, batch:1950/2500, reward:1783.5484619140625\n",
      "epoch:27, batch:2000/2500, reward:2705.323974609375\n",
      "epoch:27, batch:2050/2500, reward:3574.3505859375\n",
      "epoch:27, batch:2100/2500, reward:3629.1279296875\n",
      "epoch:27, batch:2150/2500, reward:3433.95751953125\n",
      "epoch:27, batch:2200/2500, reward:3363.580810546875\n",
      "epoch:27, batch:2250/2500, reward:3172.58447265625\n",
      "epoch:27, batch:2300/2500, reward:3141.171875\n",
      "epoch:27, batch:2350/2500, reward:2967.63134765625\n",
      "epoch:27, batch:2400/2500, reward:2741.251708984375\n",
      "epoch:27, batch:2450/2500, reward:2644.864501953125\n",
      "epoch:27, batch:2500/2500, reward:2711.37841796875\n",
      "Avg Actor 2154.2607421875 --- Avg Critic 1731.38916015625\n",
      "Epoch: 27, epoch time: 72.415min, tot time: 1.456day, L_actor: 2154.261, L_critic: 1731.389, update: False\n",
      "Save Checkpoints\n",
      "epoch:28, batch:50/2500, reward:2724.699951171875\n",
      "epoch:28, batch:100/2500, reward:2586.533203125\n",
      "epoch:28, batch:150/2500, reward:2591.3095703125\n",
      "epoch:28, batch:200/2500, reward:2632.38427734375\n",
      "epoch:28, batch:250/2500, reward:2599.257568359375\n",
      "epoch:28, batch:300/2500, reward:2684.69580078125\n",
      "epoch:28, batch:350/2500, reward:2567.11279296875\n",
      "epoch:28, batch:400/2500, reward:2546.222412109375\n",
      "epoch:28, batch:450/2500, reward:2577.04345703125\n",
      "epoch:28, batch:500/2500, reward:2469.51953125\n",
      "epoch:28, batch:550/2500, reward:2509.572021484375\n",
      "epoch:28, batch:600/2500, reward:2469.987548828125\n",
      "epoch:28, batch:650/2500, reward:2479.26611328125\n",
      "epoch:28, batch:700/2500, reward:2538.367431640625\n",
      "epoch:28, batch:750/2500, reward:2510.082763671875\n",
      "epoch:28, batch:800/2500, reward:2506.029296875\n",
      "epoch:28, batch:850/2500, reward:2465.569091796875\n",
      "epoch:28, batch:900/2500, reward:2479.25927734375\n",
      "epoch:28, batch:950/2500, reward:2486.699951171875\n",
      "epoch:28, batch:1000/2500, reward:2525.38427734375\n",
      "epoch:28, batch:1050/2500, reward:2475.934814453125\n",
      "epoch:28, batch:1100/2500, reward:2475.616943359375\n",
      "epoch:28, batch:1150/2500, reward:2467.5458984375\n",
      "epoch:28, batch:1200/2500, reward:2464.421875\n",
      "epoch:28, batch:1250/2500, reward:2436.97216796875\n",
      "epoch:28, batch:1300/2500, reward:2466.268310546875\n",
      "epoch:28, batch:1350/2500, reward:2443.37353515625\n",
      "epoch:28, batch:1400/2500, reward:2389.31201171875\n",
      "epoch:28, batch:1450/2500, reward:2352.131591796875\n",
      "epoch:28, batch:1500/2500, reward:2495.5537109375\n",
      "epoch:28, batch:1550/2500, reward:2449.393798828125\n",
      "epoch:28, batch:1600/2500, reward:2409.19384765625\n",
      "epoch:28, batch:1650/2500, reward:2374.03173828125\n",
      "epoch:28, batch:1700/2500, reward:2519.51611328125\n",
      "epoch:28, batch:1750/2500, reward:2367.13623046875\n",
      "epoch:28, batch:1800/2500, reward:2363.50048828125\n",
      "epoch:28, batch:1850/2500, reward:2366.1201171875\n",
      "epoch:28, batch:1900/2500, reward:2367.562744140625\n",
      "epoch:28, batch:1950/2500, reward:2350.58447265625\n",
      "epoch:28, batch:2000/2500, reward:2425.61376953125\n",
      "epoch:28, batch:2050/2500, reward:2438.00732421875\n",
      "epoch:28, batch:2100/2500, reward:2377.36669921875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:28, batch:2150/2500, reward:2378.84326171875\n",
      "epoch:28, batch:2200/2500, reward:2383.99365234375\n",
      "epoch:28, batch:2250/2500, reward:2392.68359375\n",
      "epoch:28, batch:2300/2500, reward:2362.95751953125\n",
      "epoch:28, batch:2350/2500, reward:2360.752685546875\n",
      "epoch:28, batch:2400/2500, reward:2531.784423828125\n",
      "epoch:28, batch:2450/2500, reward:2375.1728515625\n",
      "epoch:28, batch:2500/2500, reward:2349.141357421875\n",
      "Avg Actor 2112.968017578125 --- Avg Critic 1731.38916015625\n",
      "Epoch: 28, epoch time: 73.063min, tot time: 1.507day, L_actor: 2112.968, L_critic: 1731.389, update: False\n",
      "Save Checkpoints\n",
      "epoch:29, batch:50/2500, reward:2520.70361328125\n",
      "epoch:29, batch:100/2500, reward:2357.860595703125\n",
      "epoch:29, batch:150/2500, reward:2295.255126953125\n",
      "epoch:29, batch:200/2500, reward:2289.93896484375\n",
      "epoch:29, batch:250/2500, reward:2296.57177734375\n",
      "epoch:29, batch:300/2500, reward:2278.078125\n",
      "epoch:29, batch:350/2500, reward:2224.332763671875\n",
      "epoch:29, batch:400/2500, reward:2212.0703125\n",
      "epoch:29, batch:450/2500, reward:2194.5205078125\n",
      "epoch:29, batch:500/2500, reward:2207.72119140625\n",
      "epoch:29, batch:550/2500, reward:2228.63037109375\n",
      "epoch:29, batch:600/2500, reward:2268.720703125\n",
      "epoch:29, batch:650/2500, reward:2231.28662109375\n",
      "epoch:29, batch:700/2500, reward:2228.2333984375\n",
      "epoch:29, batch:750/2500, reward:2168.9775390625\n",
      "epoch:29, batch:800/2500, reward:2216.49755859375\n",
      "epoch:29, batch:850/2500, reward:2222.2900390625\n",
      "epoch:29, batch:900/2500, reward:2199.820556640625\n",
      "epoch:29, batch:950/2500, reward:2190.477783203125\n",
      "epoch:29, batch:1000/2500, reward:2172.164794921875\n",
      "epoch:29, batch:1050/2500, reward:2186.072509765625\n",
      "epoch:29, batch:1100/2500, reward:2152.347412109375\n",
      "epoch:29, batch:1150/2500, reward:2146.82080078125\n",
      "epoch:29, batch:1200/2500, reward:2155.055419921875\n",
      "epoch:29, batch:1250/2500, reward:2141.76025390625\n",
      "epoch:29, batch:1300/2500, reward:2157.7607421875\n",
      "epoch:29, batch:1350/2500, reward:2120.48681640625\n",
      "epoch:29, batch:1400/2500, reward:2120.13525390625\n",
      "epoch:29, batch:1450/2500, reward:2203.369384765625\n",
      "epoch:29, batch:1500/2500, reward:2265.900390625\n",
      "epoch:29, batch:1550/2500, reward:2161.65380859375\n",
      "epoch:29, batch:1600/2500, reward:2255.27392578125\n",
      "epoch:29, batch:1650/2500, reward:2186.53125\n",
      "epoch:29, batch:1700/2500, reward:2251.28955078125\n",
      "epoch:29, batch:1750/2500, reward:2383.76416015625\n",
      "epoch:29, batch:1800/2500, reward:2163.5634765625\n",
      "epoch:29, batch:1850/2500, reward:2142.23046875\n",
      "epoch:29, batch:1900/2500, reward:2159.041015625\n",
      "epoch:29, batch:1950/2500, reward:2115.5693359375\n",
      "epoch:29, batch:2000/2500, reward:2118.8037109375\n",
      "epoch:29, batch:2050/2500, reward:2186.88671875\n",
      "epoch:29, batch:2100/2500, reward:2099.22314453125\n",
      "epoch:29, batch:2150/2500, reward:2131.87353515625\n",
      "epoch:29, batch:2200/2500, reward:2128.875\n",
      "epoch:29, batch:2250/2500, reward:2105.806640625\n",
      "epoch:29, batch:2300/2500, reward:2129.66748046875\n",
      "epoch:29, batch:2350/2500, reward:2107.1748046875\n",
      "epoch:29, batch:2400/2500, reward:2104.76611328125\n",
      "epoch:29, batch:2450/2500, reward:2154.157958984375\n",
      "epoch:29, batch:2500/2500, reward:2184.422119140625\n",
      "Avg Actor 1913.1612548828125 --- Avg Critic 1731.38916015625\n",
      "Epoch: 29, epoch time: 73.986min, tot time: 1.559day, L_actor: 1913.161, L_critic: 1731.389, update: False\n",
      "Save Checkpoints\n",
      "epoch:30, batch:50/2500, reward:2086.359375\n",
      "epoch:30, batch:100/2500, reward:2071.6142578125\n",
      "epoch:30, batch:150/2500, reward:2119.65283203125\n",
      "epoch:30, batch:200/2500, reward:2100.26708984375\n",
      "epoch:30, batch:250/2500, reward:2088.3837890625\n",
      "epoch:30, batch:300/2500, reward:2080.78662109375\n",
      "epoch:30, batch:350/2500, reward:2100.0859375\n",
      "epoch:30, batch:400/2500, reward:1988.838134765625\n",
      "epoch:30, batch:450/2500, reward:1998.142578125\n",
      "epoch:30, batch:500/2500, reward:2001.26513671875\n",
      "epoch:30, batch:550/2500, reward:1999.80224609375\n",
      "epoch:30, batch:600/2500, reward:1957.767822265625\n",
      "epoch:30, batch:650/2500, reward:1969.8359375\n",
      "epoch:30, batch:700/2500, reward:1959.5662841796875\n",
      "epoch:30, batch:750/2500, reward:2087.646728515625\n",
      "epoch:30, batch:800/2500, reward:2094.935791015625\n",
      "epoch:30, batch:850/2500, reward:1973.82861328125\n",
      "epoch:30, batch:900/2500, reward:1975.4150390625\n",
      "epoch:30, batch:950/2500, reward:1971.38916015625\n",
      "epoch:30, batch:1000/2500, reward:2003.244384765625\n",
      "epoch:30, batch:1050/2500, reward:2163.27392578125\n",
      "epoch:30, batch:1100/2500, reward:2051.710693359375\n",
      "epoch:30, batch:1150/2500, reward:1990.540771484375\n",
      "epoch:30, batch:1200/2500, reward:1954.64111328125\n",
      "epoch:30, batch:1250/2500, reward:1954.39501953125\n",
      "epoch:30, batch:1300/2500, reward:1945.291259765625\n",
      "epoch:30, batch:1350/2500, reward:2010.9443359375\n",
      "epoch:30, batch:1400/2500, reward:1945.2548828125\n",
      "epoch:30, batch:1450/2500, reward:1967.2713623046875\n",
      "epoch:30, batch:1500/2500, reward:1941.2322998046875\n",
      "epoch:30, batch:1550/2500, reward:1979.3798828125\n",
      "epoch:30, batch:1600/2500, reward:1919.359375\n",
      "epoch:30, batch:1650/2500, reward:1951.7296142578125\n",
      "epoch:30, batch:1700/2500, reward:1948.9359130859375\n",
      "epoch:30, batch:1750/2500, reward:1929.172119140625\n",
      "epoch:30, batch:1800/2500, reward:1925.6572265625\n",
      "epoch:30, batch:1850/2500, reward:1910.85986328125\n",
      "epoch:30, batch:1900/2500, reward:1966.3671875\n",
      "epoch:30, batch:1950/2500, reward:1969.6461181640625\n",
      "epoch:30, batch:2000/2500, reward:1917.2540283203125\n",
      "epoch:30, batch:2050/2500, reward:1921.681640625\n",
      "epoch:30, batch:2100/2500, reward:1935.15478515625\n",
      "epoch:30, batch:2150/2500, reward:1948.434326171875\n",
      "epoch:30, batch:2200/2500, reward:1890.320556640625\n",
      "epoch:30, batch:2250/2500, reward:1914.6805419921875\n",
      "epoch:30, batch:2300/2500, reward:2007.6165771484375\n",
      "epoch:30, batch:2350/2500, reward:1943.328857421875\n",
      "epoch:30, batch:2400/2500, reward:1922.910400390625\n",
      "epoch:30, batch:2450/2500, reward:1906.180908203125\n",
      "epoch:30, batch:2500/2500, reward:1908.9185791015625\n",
      "Avg Actor 1846.66162109375 --- Avg Critic 1731.38916015625\n",
      "Epoch: 30, epoch time: 73.520min, tot time: 1.611day, L_actor: 1846.662, L_critic: 1731.389, update: False\n",
      "Save Checkpoints\n",
      "epoch:31, batch:50/2500, reward:1939.065185546875\n",
      "epoch:31, batch:100/2500, reward:1914.044189453125\n",
      "epoch:31, batch:150/2500, reward:1893.3076171875\n",
      "epoch:31, batch:200/2500, reward:1905.141845703125\n",
      "epoch:31, batch:250/2500, reward:1911.8310546875\n",
      "epoch:31, batch:300/2500, reward:1911.0675048828125\n",
      "epoch:31, batch:350/2500, reward:2042.73779296875\n",
      "epoch:31, batch:400/2500, reward:1939.9775390625\n",
      "epoch:31, batch:450/2500, reward:1978.054443359375\n",
      "epoch:31, batch:500/2500, reward:1936.3017578125\n",
      "epoch:31, batch:550/2500, reward:2000.7584228515625\n",
      "epoch:31, batch:600/2500, reward:1942.864501953125\n",
      "epoch:31, batch:650/2500, reward:1903.4215087890625\n",
      "epoch:31, batch:700/2500, reward:1893.100341796875\n",
      "epoch:31, batch:750/2500, reward:1909.3392333984375\n",
      "epoch:31, batch:800/2500, reward:1905.080810546875\n",
      "epoch:31, batch:850/2500, reward:1900.892333984375\n",
      "epoch:31, batch:900/2500, reward:1940.21728515625\n",
      "epoch:31, batch:950/2500, reward:1905.127685546875\n",
      "epoch:31, batch:1000/2500, reward:1900.76025390625\n",
      "epoch:31, batch:1050/2500, reward:1899.5849609375\n",
      "epoch:31, batch:1100/2500, reward:1916.021728515625\n",
      "epoch:31, batch:1150/2500, reward:1906.54931640625\n",
      "epoch:31, batch:1200/2500, reward:1954.3013916015625\n",
      "epoch:31, batch:1250/2500, reward:1911.4970703125\n",
      "epoch:31, batch:1300/2500, reward:1944.4935302734375\n",
      "epoch:31, batch:1350/2500, reward:1997.876953125\n",
      "epoch:31, batch:1400/2500, reward:1977.78466796875\n",
      "epoch:31, batch:1450/2500, reward:1909.2763671875\n",
      "epoch:31, batch:1500/2500, reward:1984.73291015625\n",
      "epoch:31, batch:1550/2500, reward:1896.4830322265625\n",
      "epoch:31, batch:1600/2500, reward:1907.89892578125\n",
      "epoch:31, batch:1650/2500, reward:1899.7132568359375\n",
      "epoch:31, batch:1700/2500, reward:1878.782958984375\n",
      "epoch:31, batch:1750/2500, reward:1998.022216796875\n",
      "epoch:31, batch:1800/2500, reward:1939.9022216796875\n",
      "epoch:31, batch:1850/2500, reward:1968.928466796875\n",
      "epoch:31, batch:1900/2500, reward:1908.329345703125\n"
     ]
    }
   ],
   "source": [
    "from torch.distributions.categorical import Categorical\n",
    "# visualization\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "from tqdm import tqdm_notebook\n",
    "####### my own import file ##########\n",
    "from listofpathpoint import input_handler\n",
    "import cnc_input\n",
    "#from hybrid_models import HPN\n",
    "####### my own import file ##########\n",
    "'''\n",
    "so, the models we have are TransEncoderNet,\n",
    "                            Attention\n",
    "                            LSTM\n",
    "                            HPN\n",
    "each one have initial parameters and the forward part, \n",
    "once we have the forward part, the back propagation will \n",
    "finished automatically by pytorch  \n",
    "'''\n",
    "size = 136\n",
    "TOL = 1e-3\n",
    "TINY = 1e-15\n",
    "learning_rate = 1e-4   #learning rate\n",
    "B = 256             #batch size\n",
    "B_valLoop = 20\n",
    "steps = 2500\n",
    "n_epoch = 100       # epochs\n",
    "size_rec = int(size/4)\n",
    "\n",
    "print('======================')\n",
    "print('prepare to train')\n",
    "print('======================')\n",
    "print('Hyper parameters:')\n",
    "print('learning rate', learning_rate)\n",
    "print('batch size', B)\n",
    "print('steps', steps)\n",
    "print('epoch', n_epoch)\n",
    "print('======================')\n",
    "\n",
    "'''\n",
    "instantiate a training network and a baseline network\n",
    "'''\n",
    "temp = input_handler('mother_board.json')\n",
    "'''\n",
    "X_val consisted by 'list of list of list'\n",
    "'rectangle list' 'channel list' 'point xy list' respectively\n",
    "'''\n",
    "try:\n",
    "    del Actor  # remove existing model\n",
    "    del Critic # remove existing model\n",
    "except:\n",
    "    pass\n",
    "Actor = HPN(n_feature = 2, n_hidden = 128)\n",
    "Critic = HPN(n_feature = 2, n_hidden = 128)\n",
    "optimizer = optim.Adam(Actor.parameters(), lr=learning_rate)\n",
    "\n",
    "# Putting Critic model on the eval mode\n",
    "Actor = Actor.to(device)\n",
    "Critic = Critic.to(device)\n",
    "Critic.eval()\n",
    "\n",
    "epoch_ckpt = 0\n",
    "tot_time_ckpt = 0\n",
    "\n",
    "val_mean = []\n",
    "val_std = []\n",
    "\n",
    "plot_performance_train = []\n",
    "plot_performance_baseline = []\n",
    "# recording the result of the resent epoch makes it available for future\n",
    "#*********************# Uncomment these lines to load the previous check point\n",
    "\n",
    "checkpoint_file = \"checkpoint/checkpoint_21-11-28--19-13-20-n136-gpu0.pkl\"\n",
    "checkpoint = torch.load(checkpoint_file, map_location=device)\n",
    "epoch_ckpt = checkpoint['epoch'] + 1\n",
    "tot_time_ckpt = checkpoint['tot_time']\n",
    "plot_performance_train = checkpoint['plot_performance_train']\n",
    "plot_performance_baseline = checkpoint['plot_performance_baseline']\n",
    "Critic.load_state_dict(checkpoint['model_baseline'])\n",
    "Actor.load_state_dict(checkpoint['model_train'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "\n",
    "print('Re-start training with saved checkpoint file={:s}\\n  Checkpoint at epoch= {:d} and time={:.3f}min\\n'.format(checkpoint_file,epoch_ckpt-1,tot_time_ckpt/60))\n",
    "\n",
    "\n",
    "#***********************# Uncomment these lines to load the previous check point\n",
    "\n",
    "# Main training loop\n",
    "# The core training concept mainly upon Sampling from the actor\n",
    "# then taking the greedy action from the critic\n",
    "\n",
    "\n",
    "start_training_time = time.time()\n",
    "time_stamp = datetime.datetime.now().strftime(\"%y-%m-%d--%H-%M-%S\") # Load the time stamp\n",
    "\n",
    "C = 0       # baseline => the object which the actor can compare\n",
    "R = 0       # reward\n",
    "\n",
    "zero_to_bsz = torch.arange(B, device = device) # a list contains 0 to (batch size -1)\n",
    "\n",
    "for epoch in range(0, n_epoch):\n",
    "    \n",
    "    # re-start training with saved checkpoint\n",
    "    epoch += epoch_ckpt # adding the number of the former epochs\n",
    "    # Train the model for one epoch\n",
    "    start = time.time() # record the starting time\n",
    "    Actor.train() \n",
    "    X_temp = temp.every_point()\n",
    "    X_temp = torch.FloatTensor(X_temp)\n",
    "    f_temp = input_handler('mother_board.json')\n",
    "    X = X_temp.repeat(B,1,1)\n",
    "    X = X.cuda()\n",
    "    \n",
    "    for i in range(1, steps+1): # 1 ~ 2500 steps\n",
    "        mask = torch.zeros(B,size).cuda() # use mask to make some points impossible to choose\n",
    "        R = 0\n",
    "        logprobs = 0\n",
    "        reward = 0\n",
    "        Y = X.view(B,size,2)\n",
    "        x = Y[:,0,:] #set the first point to x\n",
    "        h = None\n",
    "        c = None\n",
    "        context = None        #set Y_ini to the out corner\n",
    "        Transcontext = None\n",
    "        Y0 = None\n",
    "        # Actor Sampling phase\n",
    "        for k in range(size_rec):\n",
    "            context, Transcontext, output, h, c, _ = Actor(context,Transcontext,x=x, X_all=X, h=h, c=c, mask=mask)\n",
    "            sampler = torch.distributions.Categorical(output)\n",
    "            idx = sampler.sample()\n",
    "             #prepare for the back propagation of pytorch\n",
    "            reward, Y0,x = rectangle_process(f_temp, idx,Y,Y0, mask,k,B)\n",
    "            R += reward\n",
    "            logprobs += torch.log(output[zero_to_bsz, idx.data] + TINY)\n",
    "            ##mask[zero_to_bsz, idx.data] += -np.inf\n",
    "# critic baseline phase, use the baseline to compute the actual reward of agent at that time\n",
    "        mask = torch.zeros(B,size).cuda() # use mask to make some points impossible to choose\n",
    "        C = 0\n",
    "        baseline = 0\n",
    "        Y = X.view(B,size,2)\n",
    "        x = Y[:,0,:]\n",
    "        h = None\n",
    "        c = None\n",
    "        context = None\n",
    "        Transcontext = None\n",
    "        C0 = None\n",
    "        # compute tours for baseline without grad \"Cause we want to fix the weights for the critic\"\n",
    "        with torch.no_grad():\n",
    "            for k in range(size_rec):      \n",
    "                context, Transcontext, output, h, c, _ = Critic(context,Transcontext,x=x, X_all=X, h=h, c=c, mask=mask)\n",
    "                idx = torch.argmax(output, dim=1) # ----> greedy baseline critic\n",
    "                # prepare for the back propagation of pytorch\n",
    "                baseline, C0,x = rectangle_process(f_temp,idx,Y,C0, mask,k,B)\n",
    "                C += baseline\n",
    "        ###################\n",
    "        # Loss and backprop handling \n",
    "        ###################\n",
    "        \n",
    "        loss = torch.mean((R - C) * logprobs)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if i % 50 == 0:\n",
    "            print(\"epoch:{}, batch:{}/{}, reward:{}\".format(epoch, i, steps, R.mean().item()))\n",
    "    time_one_epoch = time.time() - start #recording the work time of one epoch\n",
    "    time_tot = time.time() - start_training_time + tot_time_ckpt\n",
    "    ###################\n",
    "    # Evaluate train model and baseline \n",
    "    # in this phase we just solve random instances with the actor and the critic\n",
    "    # compare this soluation if we get any improvment we'll transfer the actor's\n",
    "    # weights into the critic\n",
    "    ###################\n",
    "    # putting the actor in the eval mode\n",
    "    Actor.eval()\n",
    "    \n",
    "    mean_tour_length_actor = 0\n",
    "    mean_tour_length_critic = 0\n",
    "\n",
    "    for step in range(0,B_valLoop):\n",
    "        \n",
    "        # compute tour for model and baseline\n",
    "        X_temp_val = temp.every_point()\n",
    "        X_temp_val = torch.FloatTensor(X_temp_val)\n",
    "        X_val = X_temp_val.repeat(B,1,1)\n",
    "        X_val = X_val.cuda()\n",
    "        mask = torch.zeros(B,size).cuda()\n",
    "        R = 0\n",
    "        reward = 0\n",
    "        Y = X_val.view(B,size,2)\n",
    "        x = Y[:,0,:]\n",
    "        Y0 = None\n",
    "        h = None\n",
    "        c = None\n",
    "        context = None\n",
    "        Transcontext = None\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for k in range(size_rec):\n",
    "                #same as the above part\n",
    "                context, Transcontext, output, h, c, _ = Actor(context,Transcontext,x=x, X_all=X, h=h, c=c, mask=mask)\n",
    "                idx = torch.argmax(output, dim=1)\n",
    "                # prepare for the back propagation of pytorch\n",
    "                reward, Y0,x = rectangle_process(f_temp, idx,Y,Y0, mask,k,B)\n",
    "                R += reward\n",
    "        # critic baseline\n",
    "        mask = torch.zeros(B,size).cuda()\n",
    "        C = 0\n",
    "        baseline = 0\n",
    "        \n",
    "        Y = X_val.view(B,size,2)\n",
    "        x = Y[:,0,:]\n",
    "        \n",
    "        h = None\n",
    "        c = None\n",
    "        context = None\n",
    "        Transcontext = None\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for k in range(size_rec):\n",
    "                #same as the above part\n",
    "                context, Transcontext, output, h, c, _ = Critic(context,Transcontext,x=x, X_all=X, h=h, c=c, mask=mask)\n",
    "                idx = torch.argmax(output, dim=1)  \n",
    "                # prepare for the back propagation of pytorch\n",
    "                baseline, Y0,x = rectangle_process(f_temp, idx,Y,Y0, mask,k,B)\n",
    "                C += baseline\n",
    "\n",
    "        mean_tour_length_actor  += R.mean().item()\n",
    "        mean_tour_length_critic += C.mean().item()\n",
    "\n",
    "    mean_tour_length_actor  =  mean_tour_length_actor  / B_valLoop\n",
    "    mean_tour_length_critic =  mean_tour_length_critic / B_valLoop\n",
    "    # evaluate train model and baseline and update if train model is better\n",
    "\n",
    "    update_baseline = mean_tour_length_actor + TOL < mean_tour_length_critic\n",
    "\n",
    "    print('Avg Actor {} --- Avg Critic {}'.format(mean_tour_length_actor,mean_tour_length_critic))\n",
    "\n",
    "    if update_baseline:\n",
    "        Critic.load_state_dict(Actor.state_dict())\n",
    "        print('My actor is going on the right road Hallelujah :) Updated')\n",
    "    ###################\n",
    "    # Valdiation train model and baseline on 1k random TSP instances\n",
    "    ###################\n",
    "    # erased by daniel due to the 1K tsp is not the scale I want to train  \n",
    "\n",
    "    # For checkpoint\n",
    "    plot_performance_train.append([(epoch+1), mean_tour_length_actor])\n",
    "    plot_performance_baseline.append([(epoch+1), mean_tour_length_critic])\n",
    "    # compute the optimally gap ==> this is interesting because there is no LKH or other optimal algorithms \n",
    "    # for the problem like this rectangle characterized map\n",
    "    mystring_min = 'Epoch: {:d}, epoch time: {:.3f}min, tot time: {:.3f}day, L_actor: {:.3f}, L_critic: {:.3f}, update: {}'.format(\n",
    "        epoch, time_one_epoch/60, time_tot/86400, mean_tour_length_actor, mean_tour_length_critic, update_baseline)\n",
    "\n",
    "    print(mystring_min)\n",
    "    print('Save Checkpoints')\n",
    "\n",
    "    # Saving checkpoint\n",
    "    checkpoint_dir = os.path.join(\"checkpoint\")\n",
    "\n",
    "    if not os.path.exists(checkpoint_dir):\n",
    "        os.makedirs(checkpoint_dir)\n",
    "    \n",
    "    torch.save({\n",
    "        'epoch': epoch,\n",
    "        'time': time_one_epoch,\n",
    "        'tot_time': time_tot,\n",
    "        'loss': loss.item(),\n",
    "        'plot_performance_train': plot_performance_train,\n",
    "        'plot_performance_baseline': plot_performance_baseline,\n",
    "        'model_baseline': Critic.state_dict(),\n",
    "        'model_train': Actor.state_dict(),\n",
    "        'optimizer': optimizer.state_dict(),\n",
    "        },'{}.pkl'.format(checkpoint_dir + \"/checkpoint_\" + time_stamp + \"-n{}\".format(size) + \"-gpu{}\".format(gpu_id)))\n",
    "\n",
    "\n",
    "\n",
    "  \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                \n",
    "            \n",
    "                \n",
    "        \n",
    "                        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "index = int(9.34)\n",
    "table = [[index + 1,index + 2,index + 3],[index + 4,index + 5,index + 6],[index + 7,index + 8,index + 9]]\n",
    "gll = torch.as_tensor(table)\n",
    "i = gll[:,0]\n",
    "print(i)\n",
    "for j in gll:\n",
    "    print(j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "a = torch.tensor([])\n",
    "b = torch.Tensor([[4, 5, 6]])\n",
    "c = torch.tensor([[7, 8, 9]])\n",
    "d =  torch.cat((a, b, c), 0)\n",
    "print(d.shape)\n",
    "e =  torch.cat((a, b, c), 1)\n",
    "print(e.shape)\n",
    "i = d[:,0]\n",
    "print(i)\n",
    "print('0:', torch.cat((torch.Tensor([[4, 5, 6]]), d), 0))\n",
    "print(torch.cat((b, d),0).shape)\n",
    "print('1:', torch.cat((a, b, c), 1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
